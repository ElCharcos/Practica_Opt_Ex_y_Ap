{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-lbjokgDjzD"
      },
      "source": [
        "# Pr√°ctica 1 - Dilema del Prisionero Iterado\n",
        "\n",
        "## Introducci√≥n\n",
        "\n",
        "La *Teor√≠a de Juegos* es una rama de las matem√°ticas que estudia la interacci√≥n entre individuos racionales. Para su estudio, es habitual que se utilicen *juegos* o *dilemas* en los que los agentes implicados puedan escoger entre varias *acciones*. El resultado del juego depender√° de la acci√≥n propia y las de los oponentes. Un ejemplo muy conocido es el juego de *piedra, papel y tijera*.\n",
        "\n",
        "Probablemente el *juego* o *dilema* m√°s estudiado dentro de la Teor√≠a de Juegos es el [**Dilema del Prisionero**](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma). En este dilema, los jugadores tienen dos opciones: cooperar (C) o desertar (D). A la siguiente matriz se la conoce como *matriz de pagos* (*payoff matrix*), y es una forma compacta de representar un juego: por filas se representa a un jugador, y por columnas al otro.\n",
        "\n",
        "<center>\n",
        "\n",
        "|      |  C    |  D    |\n",
        "|------|-------|-------|\n",
        "|   **C**  | 2, 2  | -1, 3 |\n",
        "|   **D**  | 3, -1 |  0, 0 |\n",
        "\n",
        "</center>\n",
        "\n",
        "Las celdas interiores de la matriz establecen los pagos que se lleva cada jugador. El primer n√∫mero de la tupla es el pago al jugador *fila*, y el segundo n√∫mero el pago al jugador *columna*. Por ejemplo, si el jugador *fila* deserta (D) y el jugador *columna* coopera (C), el primero se llevar√° 3 puntos y el segundo *-1*. En verdad, los valores de la tabla no son *puntos*, es una medida sin dimensiones cuya √∫nica funci√≥n es permitirnos comparar resultados. De hecho, no existe un *√∫nico* dilema del prisionero: se pueden modificar los valores anteriores libremente, siempre que se mantengan las relaciones ordinales entre ellos.\n",
        "\n",
        "Para juegos sim√©tricos como el dilema del prisionero, suele ser m√°s conveniente utilizar la *matriz de pagos simplificada*\n",
        "\n",
        "<center>\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "2 & -1 \\\\\n",
        "3 & 0\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "No olvides que los jugadores del dilema del prisionero son jugadores racionales. Adem√°s, **el objetivo no es *ganar* al rival, sino maximizar el beneficio personal**. Estas dos premisas son las hip√≥tesis sobre las que se sustenta todo lo que viene a continuaci√≥n.\n",
        "\n",
        "Cuando se juega al dilema del prisionero una √∫nica vez, la estrategia √≥ptima es la de desertar (ejercicio: reflexiona sobre el por qu√©). Sin embargo, la complejidad surge cuando los jugadores se enfrentan repetidamente con el mismo oponente, ya que sus acciones actuales influir√°n en su reputaci√≥n a largo plazo. Esta variante se conoce como el **Dilema del Prisionero Iterado** y ha sido objeto de profundo estudio en la literatura cient√≠fica. A pesar de que John von Neumann fue el primero en plantear este dilema, Robert Axelrod ha destacado como uno de los cient√≠ficos m√°s influyentes en su an√°lisis.\n",
        "\n",
        "Axelrod plante√≥ varios \"campeonatos computacionales\", en los que los participantes deb√≠an proponer una estrategia para jugar al DPI. Luego Axelrod enfrent√≥ todas las estrategias y vio cu√°les eran las que mejor funcionaban. Una *estrategia* para jugar al DPI es un conjunto de instrucciones inequ√≠vocas que, a partir del hist√≥rico de movimientos de ambos jugadores, dice cu√°l debe ser el siguiente movimiento. Por ejemplo, lo siguiente ser√≠a una estrategia: \"comienzo cooperando, luego deserto dos veces, y a partir de la cuarta ronda coopero siempre\". Algunas de estas estrategias se han hecho famosas por ser exitosas o por aparece con frecuencia en la literatura.\n",
        "\n",
        "Axelrod comenz√≥ estudiando torneos *normales* (llamados *de enfrentamiento directo*), donde cada participante (estrategia) es enfrentado contra cada uno de los dem√°s, sumando los resultados obtenidos en cada interacci√≥n, y estableciendo un ranking al final del campeonato. Tambi√©n estudi√≥ los llamados *torneos evolutivos*. En ellos cada estrategia parte con un n√∫mero de individuos representantes que juegan siempre esa estrategia. Tras enfrentar todos los individuos contra todos, se elimina de la poblaci√≥n los que peor resultado han obtenido, sustituy√©ndolos por los m√°s exitosos. Este proceso se repite varias veces, dando lugar (en muchos casos) a que una estrategia coloniza a la poblaci√≥n completa: se encuentra la estrategia √≥ptima. En escenarios m√°s avanzados, se permite tambi√©n la evoluci√≥n de las estrategias a lo largo de las generaciones, mediante mecanismos de mutaci√≥n y cruce (selecci√≥n natural).\n",
        "\n",
        "Las metaheur√≠sticas son t√©cnicas de optimizaci√≥n confeccionadas para casos particulares, complejos y din√°micos. El DPI evolutivo es ejemplo tangible de esta t√©cnica. En este problema, m√∫ltiples soluciones compiten y evolucionan hacia la optimizaci√≥n de un objetivo dado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3IuFnKvDjzI"
      },
      "source": [
        "## Descripci√≥n de la pr√°ctica\n",
        "\n",
        "**¬°Importante!** Comienza jugando al juego de Nicky Case llamado *The evolution of trust* (~30min). Es un buen resumen de los conceptos previos que necesitas saber sobre el DPI antes de comenzar la pr√°ctica:\n",
        "<center>\n",
        "\n",
        "[<img src=\"https://ncase.me/img/trust.png\" alt=\"The evolution of trust: https://ncase.me/trust/\" width=\"400\">](https://ncase.me/trust/)\n",
        "\n",
        "https://ncase.me/trust/\n",
        "</center>\n",
        "Unas aclaraciones sobre el juego: en ocasiones, el autor utiliza una nomenclatura que no es habitual en la literatura del DPI. Por evitar confusiones:\n",
        "\n",
        " - Las acciones posibles en el dilema las llamaremos Cooperar (*Cooperate*) y Desertar (*Defect*), y no *cheat*.\n",
        " - La estrategia que repite el √∫ltimo movimiento del rival se conoce ampliamente como *Tit-For-Tat*, y no *Copycat*.\n",
        "\n",
        "La pr√°ctica tiene dos partes:\n",
        " - **Parte 1**: Montar la estructura computacional que permita simular torneos del DPI.\n",
        " - **Parte 2**: Dise√±ar una estrategia, que se enfrentar√° a las de tus compa√±eros en un torneo de tres fases descrito a continuaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93D6PBnyDjzJ"
      },
      "source": [
        "## Parte 1: Sofware para el estudio del DPI\n",
        "\n",
        "El primer objetivo es crear un sofware que permita simular torneos del DPI. En concreto, el software debe cumplir los siguientes requisitos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUxRfYmHDjzK"
      },
      "source": [
        " - Para el torneo de enfrentamiento directo, las entradas al programa deben ser:\n",
        "   - ```all_players```: lista o tupla de jugadores que van a participar en el campeonato (con informaci√≥n sobre sus estrategias).\n",
        "   - ```dilemma```: objeto para definir el dilema del prisionero jugado (en particular, deber√° contener la informaci√≥n de la correspondiente matriz de pagos, que es la mostrada arriba).\n",
        "   - ```n_rounds```: n√∫mero de rondas por cada enfrentamiento.\n",
        "   - ```error```: probabilidad de error por jugada.\n",
        "   - ```repetitions```: el n√∫mero de veces que una estrategia va a enfrentarse a otra.\n",
        " - La salida debe ser la informaci√≥n (visual) sobre el resultado del campeonato.\n",
        " - Adem√°s, el programa deber√° contener al menos las 5 estrategias b√°sicas descritas en el juego de Nicky Case. Igualmente, deber√° permitir a√±adir nuevas estrategias de forma sencilla.\n",
        "\n",
        "Consulta la secci√≥n sobre el *m√≥dulo evolutivo* para ver las particularidades de esa parte.\n",
        "\n",
        "A continuaci√≥n se propone una **plantilla de desarrollo** con los m√≥dulos/clases que se recomienda implementar para resolver este problema. No es obligatorio seguir esta estructura. Por otro lado, puede ser recomendable organizar el c√≥digo en un proyecto local en lugar de en *Jupyter* o *Colab*. Si quieres hacerlo, la estructura del proyecto recomendada es la siguiente:\n",
        "\n",
        "```\n",
        " DPI/\n",
        "   ‚îú‚îÄ‚îÄ dpi/\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ dilemma.py  # implementa la l√≥gica del DP jugado, acciones posibles, matriz de pagos, etc\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ player.py  # implementa la definici√≥n de \"Jugador\" y en particular, la estrategia\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ game.py  # implementa la din√°mica de un juego cara a cara entre dos jugadores\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ tournament.py  # implementa la din√°mica del torneo general\n",
        "   ‚îÇ   ‚îî‚îÄ‚îÄ evolution.py  # implementa la din√°mica del torneo evolutivo\n",
        "   ‚îú‚îÄ‚îÄ main.py # main del proyecto - si quieres, usa varios 'mains' para testear los distintos m√≥dulos\n",
        "   ‚îî‚îÄ‚îÄ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXaTlPU4DjzM"
      },
      "source": [
        "### Plantilla de desarrollo\n",
        "\n",
        "Se incluye a continuaci√≥n una plantilla con los m√≥dulos/clases que se recomienda implementar, junto con variables y m√©todos necesarios. El objetivo es facilitar la tarea de dise√±ar el c√≥digo. No es obligatorio seguir esta estructura, pero a falta de mejores ideas, puede ser un buen punto de partida.\n",
        "\n",
        "Para facilitar la tarea de identificar qu√© m√©todos faltan por implementar, en todos ellos se ha incluido la excepci√≥n ```raise NotImplementedError``` para poner de manifiesto que dicho m√©todo debe ser implementado.\n",
        "\n",
        "Se comienza importando librer√≠as. El uso de otras librer√≠as **basicas** tambi√©n est√° permitido, siempre que est√© justificado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\charly\\anaconda3\\lib\\site-packages (1.24.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\charly\\anaconda3\\lib\\site-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "jOQdcOwWDjzO"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import typing\n",
        "import itertools\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w41RjVoMDjzP"
      },
      "source": [
        "#### El m√≥dulo ```dilemma```\n",
        "\n",
        "El primer m√≥dulo que se recomienda implementar es el que recoge la informaci√≥n sobre el dilema jugado. Como el DP solo tiene dos acciones posibles (*Cooperate* y *Defect*), por simplicidad crearemos dos variables globales ```C``` y ```D``` asociadas a dos enteros (0 y 1 respectivamente). Representar√°n estas dos acciones a lo largo de todo el c√≥digo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "mvUPibN8DjzQ"
      },
      "outputs": [],
      "source": [
        "C = 0\n",
        "D = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udrWfLuQDjzR"
      },
      "source": [
        "Ahora se propone implementar una clase llamada ```Dilemma``` para representar dilemas sim√©tricos 2x2, como el dilema del prisionero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "9esWjcgnDjzS"
      },
      "outputs": [],
      "source": [
        "class Dilemma:\n",
        "\n",
        "    def __init__(self, cc: float, cd: float, dc: float, dd: float):\n",
        "        \"\"\"\n",
        "        Represents a 2x2 symmetric dilemma.\n",
        "\n",
        "        Parameters:\n",
        "            - cc (float): payoff for mutual cooperation\n",
        "            - cd (float): payoff when one cooperates, but the opponent defects\n",
        "            - dc (float): payoff when one defects, and the opponent cooperates\n",
        "            - dd (float): payoff for mutual defection\n",
        "        \"\"\"\n",
        "        self.dilema_matrix = np.array([np.array([cc, cd]), np.array([dc, dd])])\n",
        "\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def payoff_matrix(self) -> npt.NDArray[np.floating]:\n",
        "        \"\"\"\n",
        "        Symmetric pay-off matrix of the dilema\n",
        "\n",
        "        Returns:\n",
        "            - 2x2 np array of the matrix\n",
        "        \"\"\"\n",
        "        return self.dilema_matrix\n",
        "\n",
        "    @abstractmethod\n",
        "    def evaluate_result(self, a_1: int, a_2: int) -> tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Given two actions, returns the payoffs of the two players.\n",
        "\n",
        "        Parameters:\n",
        "            - a_1 (int): action of player 1 ('C' or 'D', i.e. '1' or '0')\n",
        "            - a_2 (int): action of player 2 ('C' or 'D', i.e. '1' or '0')\n",
        "\n",
        "        Returns:\n",
        "            - tuple of two floats, being the first and second values the payoff\n",
        "            for the first and second player, respectively.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.dilema_matrix[a_1, a_2], self.dilema_matrix[a_2, a_1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjwkGv5xDjzT"
      },
      "source": [
        "Comprueba que los m√©todos anteriores funcionan correctamente con el siguiente test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvQe2DeiDjzU",
        "outputId": "6b231a03-6e06-4dcc-ad1d-25af301b772f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, 0) -> (2, 2)\n",
            "(0, 1) -> (-1, 3)\n",
            "(1, 0) -> (3, -1)\n",
            "(1, 1) -> (0, 0)\n"
          ]
        }
      ],
      "source": [
        "#Prints all possible outcomes of the PD\n",
        "pd = Dilemma(2, -1, 3, 0)\n",
        "posible_actions = (C, D)\n",
        "for a1, a2 in itertools.product(posible_actions, repeat=2):\n",
        "    print(f\"{(a1, a2)} -> {pd.evaluate_result(a1, a2)}\")\n",
        "\n",
        "# Output:\n",
        "# (0, 0) -> (2, 2)\n",
        "# (0, 1) -> (-1, 3)\n",
        "# (1, 0) -> (3, -1)\n",
        "# (1, 1) -> (0, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSXC4GF_DjzU"
      },
      "source": [
        "#### El m√≥dulo ```player```\n",
        "\n",
        "Ahora vamos a programar una m√≥dulo que sirva para representar a los jugadores. En particular, su m√©todo principal ser√° ```strategy()```, que devolver√° una acci√≥n a realizar, en base a la historia de interacci√≥n con otro jugador. Adem√°s, aprovecharemos a implementar los *jugadores ilustres*, aquellos que juegan estrategias famosas, para que luego sea sencillo hacer pruebas.\n",
        "\n",
        "Empezaremos programando una clase abstracta llamada ```player``` de la que heredar√°n los jugadores concretos que programaremos a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "qY4GXEjnDjzV"
      },
      "outputs": [],
      "source": [
        "class Player(ABC):\n",
        "\n",
        "    # Este m√©todo ya est√° implementado\n",
        "    @abstractmethod\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"\"):\n",
        "        \"\"\"\n",
        "        Abstract class that represents a generic player\n",
        "\n",
        "        Parameters:\n",
        "            - name (str): the name of the strategy\n",
        "            - dilemma (Dilemma): the dilemma that this player will play\n",
        "        \"\"\"\n",
        "\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "\n",
        "        self.history  = []  # This is the main variable of this class. It is\n",
        "                            # intended to store all the history of actions\n",
        "                            # performed by this player.\n",
        "                            # Example: [C, C, D, D, D] <- So far, the\n",
        "                            # interaction lasts five rounds. In the first one,\n",
        "                            # this player cooperated. In the second, he also\n",
        "                            # cooperated. In the third, he defected. Etc.\n",
        "\n",
        "    def __str__(self):\n",
        "       return self.name\n",
        "\n",
        "    # Este m√©todo ya est√° implementado\n",
        "    @abstractmethod\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"\n",
        "        Main call of the class. Gives the action for the following round of the\n",
        "        interaction, based on the history\n",
        "\n",
        "        Parameters:\n",
        "            - opponent (Player): is another instance of Player.\n",
        "\n",
        "        Results:\n",
        "            - An integer representing Cooperation (C=0) or Defection (D=1)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "    def compute_scores(self, opponent: Player) -> tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Compute the scores for a given opponent\n",
        "\n",
        "        Parameters:\n",
        "            - opponent (Player): is another instance of Player.\n",
        "\n",
        "        Results:\n",
        "            - A tuple of two floats, where the first value is the current\n",
        "            player's payoff, and the second value is the opponent's payoff.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # Este m√©todo ya est√° implementado\n",
        "    def clean_history(self):\n",
        "        \"\"\"Resets the history of the current player\"\"\"\n",
        "        self.history = []\n",
        "\n",
        "\n",
        "# A continuaci√≥n se representan las 5 estrategias b√°sicas del juego de Nicky Case\n",
        "\n",
        "class Cooperator(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"Smooth Cooperator\"):\n",
        "        \"\"\"Cooperator\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Cooperates always\"\"\"\n",
        "        self.history.append(C)\n",
        "        return C\n",
        "\n",
        "\n",
        "\n",
        "class Defector(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"Defector\"):\n",
        "        \"\"\"Defector\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Defects always\"\"\"\n",
        "        self.history.append(D)\n",
        "        return D\n",
        "\n",
        "\n",
        "class Tft(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"Tit-For-Tat\"):\n",
        "        \"\"\"Tit-for-tat\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Cooperates first, then repeat last action of the opponent\"\"\"\n",
        "\n",
        "        if len(opponent.history) > 0:\n",
        "          # repeat last action of the opponent\n",
        "          opponents_last_action = opponent.history[-1]\n",
        "          self.history.append(opponents_last_action)\n",
        "          return opponents_last_action\n",
        "        else:\n",
        "          self.history.append(C)\n",
        "          return C\n",
        "\n",
        "class Grudger(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"Grudger\"):\n",
        "        \"\"\"Grudger\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"\n",
        "        Cooperates always, but if opponent ever defects, it will defect for the\n",
        "        rest of the game\n",
        "        \"\"\"\n",
        "        # cooperates by default\n",
        "        action = C\n",
        "\n",
        "        # upon deflection, starts deflecting, checks its own history to\n",
        "        # maintain decision\n",
        "        if len(opponent.history) > 0 and len(self.history) > 0:\n",
        "          if opponent.history[-1] == D or self.history[-1] == D:\n",
        "            action = D\n",
        "            \n",
        "        self.history.append(action)\n",
        "        return action\n",
        "\n",
        "class Detective4MovsTft(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"4M TFT Detective\"):\n",
        "        \"\"\"Four movement - tit for tat detective\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"\n",
        "        Starts with a fixed sequence of actions: [C,D,C,C]. After that, if\n",
        "        the opponent has ever defected, plays 'TFT'. If not, plays 'Defector'.\n",
        "        \"\"\"\n",
        "        match len(self.history):\n",
        "          case 0:\n",
        "            result = C\n",
        "          case 1:\n",
        "            result = D\n",
        "          case 2:\n",
        "            result = C\n",
        "          case 3:\n",
        "            result = C\n",
        "          case _:\n",
        "            # On tournament, when Detective has history but\n",
        "            # opponent does not have because it has not played\n",
        "            # an error is thrown. With this, we get rid of\n",
        "            # the error and select the \"C\" which is the \n",
        "            # first option for detective\n",
        "            if len(opponent.history) == 0:\n",
        "              result = C\n",
        "            else:\n",
        "              result = opponent.history[-1]\n",
        "\n",
        "        self.history.append(result)\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [],
      "source": [
        "class T10(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"T10\"):\n",
        "        \"\"\"T10\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "        self.point = 0\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Cooperates first, then use TfT if we are wining, else we change to Grudger or Win‚Äìstay, lose‚Äìswitch if we are losing\"\"\"\n",
        "\n",
        "        turn = len(self.history)             # Current Turn\n",
        "        count_D = opponent.history.count(D)  # Num. of defections of the oponent\n",
        "        count_C = opponent.history.count(C)  # Num. of cooperations of the oponent\n",
        "        #Point calculator\n",
        "        if len(self.history) > 0 and len(opponent.history) > 0:\n",
        "            self.point += pd.evaluate_result(self.history[-1], opponent.history[-1])[0]\n",
        "\n",
        "        #We start at turn 1 cooperating\n",
        "        if turn == 0 or turn == 1:\n",
        "            self.history.append(C)\n",
        "            return C  \n",
        "        \n",
        "        if turn == 10 and (opponent.history[-1] == 0 and opponent.history[-2] == 0 and opponent.history[-3] == 0):\n",
        "            self.history.append(D)\n",
        "            return D \n",
        "        \n",
        "        #If at mid round we are losing we use Grudger or Win‚Äìstay until end match\n",
        "        if 10 >= turn >= 5 and self.point <= 10:\n",
        "            self.point =0\n",
        "            if count_D > count_C:\n",
        "            #Grudger\n",
        "                action = C\n",
        "                if len(opponent.history) > 0 and len(self.history) > 0:\n",
        "                    if opponent.history[-1] == D or self.history[-1] == D:\n",
        "                        action = D\n",
        "                return action\n",
        "            \n",
        "            else:\n",
        "                # Win‚Äìstay, lose‚Äìswitch\n",
        "                last_round = self.history[-1], opponent.history[-1]\n",
        "                if last_round[0] == C and last_round[1] == D:\n",
        "                    self.history.append(D)\n",
        "                    return D # Switch before lose\n",
        "                else:\n",
        "                    self.history.append(C)\n",
        "                    return C # Stil cooperates before winning\n",
        "        \n",
        "        #Until that (if we are winning), we follow TfT\n",
        "        if 5 >= turn > 1 or (10 >= turn > 5 and self.point > 10):\n",
        "            opponents_last_action = opponent.history[-1]\n",
        "            self.history.append(opponents_last_action)\n",
        "            return opponents_last_action\n",
        "\n",
        "        else:\n",
        "            opponents_last_action = opponent.history[-1]\n",
        "            self.history.append(opponents_last_action)\n",
        "            return opponents_last_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4qR0bgwDjzW"
      },
      "source": [
        "Testearemos este m√≥dulo una vez programemos el siguiente\n",
        "\n",
        "#### El m√≥dulo ```game```\n",
        "\n",
        "Ya sabemos manejar jugadores. Ahora vamos a enfrentarles. El presente m√≥dulo pretende recoger las herramientas necesarias para enfrentar a dos jugadores en una *partida* del dilema del prisionero iterado. Est√° compuesto por una clase llamada ```Game``` que implementa el m√©todo principal ```play()```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "monHjo20DjzW"
      },
      "outputs": [],
      "source": [
        "class Game:\n",
        "\n",
        "    # Este m√©todo ya est√° implementado\n",
        "    def __init__(self, player_1: Player,\n",
        "                       player_2: Player,\n",
        "                       n_rounds: int = 100,\n",
        "                       error: float = 0.0):\n",
        "        \"\"\"\n",
        "        Game class to represent an iterative dilema\n",
        "\n",
        "        Parameters:\n",
        "            - player_1 (Player): first player of the game\n",
        "            - player_2 (Player): second player of the game\n",
        "            - n_rounds (int = 100): number of rounds in the game\n",
        "            - error (float = 0.0): error probability (in base 1)\n",
        "        \"\"\"\n",
        "\n",
        "        assert n_rounds > 0, \"'n_rounds' should be greater than 0\"\n",
        "\n",
        "        self.player_1 = player_1\n",
        "        self.player_2 = player_2\n",
        "        self.n_rounds = n_rounds\n",
        "        self.error = error\n",
        "\n",
        "        self.score = (0.0, 0.0)  # this variable will store the final result of\n",
        "                                 # the game, once the 'play()' function has\n",
        "                                 # been called. The two values of the tuple\n",
        "                                 # correspond to the points scored by the first\n",
        "                                 # and second player, respectively.\n",
        "\n",
        "\n",
        "    def play(self, do_print: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Main call of the class. Play the game.\n",
        "        Stores the final result in 'self.score'\n",
        "\n",
        "        Parameters\n",
        "            - do_print (bool = False): if True, should print the ongoing\n",
        "            results at the end of each round (i.e. print round number, last\n",
        "            actions of both players and ongoing score).\n",
        "        \"\"\"\n",
        "        for round in range(1, self.n_rounds+1):\n",
        "          player1_move = self.player_1.strategy(self.player_2)\n",
        "          player2_move = self.player_2.strategy(self.player_1)\n",
        "          round_result = pd.evaluate_result(player1_move, player2_move)\n",
        "          self.score = tuple(map(sum, zip(self.score, round_result)))\n",
        "\n",
        "          if do_print == True:\n",
        "            print(f'===== ROUND {round} =====')\n",
        "            print(f'[üòá] Player 1 ({str(self.player_1)}) decided: {player1_move}')\n",
        "            print(f'[üòà] Player 2 ({str(self.player_2)}) decided: {player2_move}')\n",
        "            print(f'[üìã] Round result: {round_result}')\n",
        "            print(f'[üìÖ] Current score: {self.score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJATrCGwDjzX"
      },
      "source": [
        "Comprueba que los √∫ltimos dos m√≥dulos funcionan como se espera en la siguiente celda. Experimenta con distintas combinaciones de jugadores y observa que los resultados son los esperados. Por ejemplo, un Cooperador y un Grudger se pasan toda su interacci√≥n cooperando, por lo que en un juego de 10 rondas sin error, ambos deben obtener 20 puntos al final. Comprueba tambi√©n que el error se est√° tratando como corresponde. Por ejemplo, f√≠jalo en un valor alto (e.g. 0.2) y comprueba que en el enfrentamiento de un Cooperador con un Grudger alguno deserta en alguna ocasi√≥n. Comprueba tambi√©n que puedes enfrentar dos jugadores que jueguen la misma estrategia (simplemente crea dos instancias del mismo jugador, con distintos nombres)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XfwcYqIDjzX",
        "outputId": "ee30dc60-7f4b-46a5-8b29-55e5c1a2da91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== ROUND 1 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (2.0, 2.0)\n",
            "===== ROUND 2 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (4.0, 4.0)\n",
            "===== ROUND 3 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (6.0, 6.0)\n",
            "===== ROUND 4 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (8.0, 8.0)\n",
            "===== ROUND 5 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (10.0, 10.0)\n",
            "===== ROUND 6 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (12.0, 12.0)\n",
            "===== ROUND 7 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (14.0, 14.0)\n",
            "===== ROUND 8 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (16.0, 16.0)\n",
            "===== ROUND 9 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (18.0, 18.0)\n",
            "===== ROUND 10 =====\n",
            "[üòá] Player 1 (T10) decided: 0\n",
            "[üòà] Player 2 (cooperator) decided: 0\n",
            "[üìã] Round result: (2, 2)\n",
            "[üìÖ] Current score: (20.0, 20.0)\n"
          ]
        }
      ],
      "source": [
        "dilemma = Dilemma(2, -1, 3, 0)\n",
        "\n",
        "cooperator_player = Cooperator(dilemma, \"cooperator\")\n",
        "defector_player = Defector(dilemma, \"defector\")\n",
        "tft_player = Tft(dilemma, \"tft\")\n",
        "grudger_player = Grudger(dilemma, \"grudger\")\n",
        "detective_player = Detective4MovsTft(dilemma, \"detective\")\n",
        "el_t10 = T10(dilemma, \"T10\")\n",
        "\n",
        "#Modifica las siguientes l√≠neas a conveniencia para llevar a cabo distintos tests\n",
        "game = Game(el_t10, cooperator_player, n_rounds=10, error=0.2)\n",
        "game.play(do_print=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBb8_qsjDjzY"
      },
      "source": [
        "#### El m√≥dulo ```torunament```\n",
        "\n",
        "Finalmente, llegamos al m√≥dulo que nos va a permitir simular el campeonato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "OgivyVhwDjzY"
      },
      "outputs": [],
      "source": [
        "class Tournament:\n",
        "\n",
        "    # Este m√©todo ya est√° implementado\n",
        "    def __init__(self, players: tuple[Player, ...],\n",
        "                       n_rounds: int = 100,\n",
        "                       error: float = 0.0,\n",
        "                       repetitions: int = 2):\n",
        "        \"\"\"\n",
        "        All-against-all tournament\n",
        "\n",
        "        Parameters:\n",
        "            - players (tuple[Player, ...]): tuple of players that will play the\n",
        "         tournament\n",
        "            - n_rounds (int = 100): number of rounds in the game\n",
        "            - error (float = 0.0): error probability (in base 1)\n",
        "            - repetitions (int = 2): number of games each player plays against\n",
        "         the rest\n",
        "        \"\"\"\n",
        "\n",
        "        self.players = players\n",
        "        self.n_rounds = n_rounds\n",
        "        self.error = error\n",
        "        self.repetitions = repetitions\n",
        "\n",
        "        # This is a key variable of the class. It is intended to store the\n",
        "        # ongoing ranking of the tournament. It is a dictionary whose keys are\n",
        "        # the players in the tournament, and its corresponding values are the\n",
        "        # points obtained in their interactions with each other. In the end, to\n",
        "        # see the winner, it will be enough to sort this dictionary by the\n",
        "        # values.\n",
        "        self.ranking = {player: 0.0 for player in self.players}  # initial vals\n",
        "\n",
        "\n",
        "    def sort_ranking(self) -> None:\n",
        "        \"\"\"Sort the ranking by the value (score)\"\"\"\n",
        "        # aqui prefiero castear todo tras comprobar el isinstance pero bueno parche rapido\n",
        "        self.ranking = dict(sorted(self.ranking.items(), key=lambda item: item[1] if isinstance(item[1], (int, float)) else 0, reverse=True))\n",
        "\n",
        "    #pista: utiliza 'itertools.combinations' para hacer los cruces\n",
        "    def play(self) -> None:\n",
        "        \"\"\"\n",
        "        Main call of the class. It must simulate the championship and update\n",
        "        the variable 'self.ranking' with the accumulated points obtained by\n",
        "        each player in their interactions.\n",
        "        \"\"\"\n",
        "        for player1, player2 in itertools.combinations(self.players, 2):\n",
        "            game = Game(player1, player2, self.n_rounds, self.error)\n",
        "            for _ in range(self.repetitions):\n",
        "                game.play()\n",
        "                self.update_ranking(player1, player2, game.score)\n",
        "\n",
        "    def update_ranking(self, player1, player2, game_score):\n",
        "      \"\"\"Funcion auxiliar para actualizar el ranking\"\"\"\n",
        "      self.ranking[player1] += game_score[0]\n",
        "      self.ranking[player2] += game_score[1]\n",
        "\n",
        "    def plot_results(self):\n",
        "      \"\"\"\n",
        "      Plots a bar chart of the final ranking. On the x-axis should appear\n",
        "      the names of the sorted ranking of players participating in the\n",
        "      tournament. On the y-axis the points obtained.\n",
        "      \"\"\"\n",
        "      self.sort_ranking()\n",
        "\n",
        "      players = list(self.ranking.keys())  # Obtiene las claves (jugadores)\n",
        "      points = list(self.ranking.values())  # Obtiene los valores (puntos)\n",
        "\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.barh([player.name for player in players], [float(point) for point in points])  # Convertir los puntos a flotantes\n",
        "      plt.xlabel('Points')\n",
        "      plt.ylabel('Players')\n",
        "      plt.title('Final Ranking')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92h2VDkXDjzZ"
      },
      "source": [
        "Para testear la implementaci√≥n, por ejemplo prueba a reproducir el campeonato de la secci√≥n \"3. One Tournament\" del juego de Nicky Case. En dicho campeonato se enfrentaban las 5 estrategias b√°sicas (cooperador, desertor, tft, grudger y detective) en un torneo con 10 rondas por interacci√≥n y sin error. Los resultados que debes obtener son los siguientes:\n",
        " - Tit-For-Tat: ganador con 57 puntos\n",
        " - Grudger: 46 puntos\n",
        " - Detective: 45 puntos\n",
        " - Desertor: 45 puntos\n",
        " - Cooperador: 29 puntos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "Jl5Zb7KsDjzZ"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dilemma = Dilemma(2, -1, 3, 0)\n",
        "\n",
        "cooperator_player = Cooperator(dilemma, \"cooperator\")\n",
        "defector_player = Defector(dilemma, \"defector\")\n",
        "tft_player = Tft(dilemma, \"tft\")\n",
        "grudger_player = Grudger(dilemma, \"grudger\")\n",
        "detective_player = Detective4MovsTft(dilemma, \"detective\")\n",
        "el_t10 = T10(dilemma, \"T10\")\n",
        "\n",
        "all_players = (cooperator_player, defector_player, tft_player, grudger_player,\n",
        "               detective_player, el_t10)\n",
        "\n",
        "tournament = Tournament(all_players, n_rounds=10, error=0.0, repetitions=1)\n",
        "tournament.play()\n",
        "tournament.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CChTfIl9Djza"
      },
      "source": [
        "#### El m√≥dulo ```evolution```\n",
        "\n",
        "Implementa tambi√©n la variante evolutiva del torneo, similar a la que se explica en la secci√≥n \"4. Repeated Tournament\" del juego de Nicky Case. Este m√≥dulo necesitar√° de inputs extra. Ten en cuenta lo siguiente:\n",
        " - La poblaci√≥n inicial de jugadores no es directamente el input de jugadores que d√© el usuario, sino que cada jugador tiene varios \"individuos\" o \"r√©plicas\" que jugar√°n su estrategia. Se propone dar al usuario dos opciones para definir esta poblaci√≥n inicial:\n",
        "   - Si el usuario define el tama√±o de la poblaci√≥n total (```int```), se asume que cada jugador comienza con el mismo n√∫mero de representantes. Divide ese n√∫mero entre el n√∫mero de jugadores (redondeado al entero m√°s pr√≥ximo) y as√≠ obtendr√°s el n√∫mero de individuos inicial de cada estrategia.\n",
        "   - Si el usuario define una tupla de n√∫meros de individuos (```list[int, ...]```), se asume que cada jugador tendr√° el n√∫mero de representantes que indique su √≠ndice dentro de dicha tupla.\n",
        " - Hay dos par√°metros m√°s que controlan el proceso evolutivo.\n",
        "   - En primer lugar, el porcentaje de individuos que se desea incluir en la selecci√≥n natural tras cada ronda; esto es, el n√∫mero de individuos *de la parte de abajo del ranking* que se van a eliminar y sustituir por los *individuos de la parte de arriba* (en caso de empate entre individuos, escoge al azar).\n",
        "   - En segundo lugar, el n√∫mero de generaciones que se van a simular. Una generaci√≥n es b√°sicamente un \"Torneo de enfrentamiento directo\" + un \"Proceso de selecci√≥n natural\".\n",
        "\n",
        "Deber√°s tomar una decisi√≥n sobre qu√© hacer en la selecci√≥n natural cuando hay *empates*.\n",
        "\n",
        "A continuaci√≥n se incluye la plantilla de desarrollo sugerida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "gj9E4kqJDjza"
      },
      "outputs": [],
      "source": [
        "class Evolution:\n",
        "\n",
        "    # Este m√©todo ya est√° implementado\n",
        "    def __init__(self, players: tuple[Player, ...],\n",
        "                       n_rounds: int = 100,\n",
        "                       error: float = 0.0,\n",
        "                       repetitions: int = 2,\n",
        "                       generations: int = 100,\n",
        "                       reproductivity: float = 0.05,\n",
        "                       initial_population: tuple[int, ...] | int = 100):\n",
        "        \"\"\"\n",
        "        Evolutionary tournament\n",
        "\n",
        "        Parameters:\n",
        "            - players (tuple[Player, ...]): tuple of players that will play the\n",
        "         tournament\n",
        "            - n_rounds (int = 100): number of rounds in each game\n",
        "            - error (float = 0.0): error probability (in base 1)\n",
        "            - repetitions (int = 2): number of games each player plays against\n",
        "         the rest\n",
        "            - generations (int = 100): number of generations to simulate\n",
        "            - reproductivity (float = 0.05): ratio (base 1) of worst players\n",
        "         that will be removed and substituted by the top ones in the natural\n",
        "         selection process carried out at the end of each generation\n",
        "            - initial_population (tuple[int, ...] | int = 100): list of\n",
        "         individuals representing each players (same index as 'players' tuple)\n",
        "         OR total population size (int).\n",
        "        \"\"\"\n",
        "\n",
        "        self.players = players\n",
        "        self.n_rounds = n_rounds\n",
        "        self.error = error\n",
        "        self.repetitions = repetitions\n",
        "        self.generations = generations\n",
        "        self.reproductivity = reproductivity\n",
        "\n",
        "        if isinstance(initial_population, int):\n",
        "            self.initial_population = [math.floor(initial_population\n",
        "                                       / len(self.players))\n",
        "                                       for _ in range(len(self.players))]\n",
        "        else:\n",
        "            self.initial_population = initial_population\n",
        "\n",
        "        self.total_population = sum(self.initial_population)\n",
        "        self.repr_int = int(self.total_population * self.reproductivity)\n",
        "\n",
        "        self.ranking = {copy.deepcopy(player): 0.0 for i, player in\n",
        "                        enumerate(self.players)\n",
        "                        for _ in range(self.initial_population[i])}\n",
        "\n",
        "\n",
        "    def natural_selection(self, result_tournament: dict[Player, float]) -> dict:\n",
        "      \"\"\"\n",
        "      Kill the worst guys, reproduce the top ones. Takes the ranking once a\n",
        "      face-to-face tournament has been played and returns another ranking,\n",
        "      with the evolutionary changes applied\n",
        "\n",
        "      Parameters:\n",
        "          - result_tournament: the 'tournament.ranking' kind of dict.\n",
        "\n",
        "      Results:\n",
        "          - Same kind of dict ranking as the input, but with the evolutionary\n",
        "          dynamics applied\n",
        "      \"\"\"\n",
        "\n",
        "      # no s√© si asumir un diccionario ordenado pero por si acaso, luego se puede borrar\n",
        "      sorted_players = sorted(result_tournament.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      top_players = dict(sorted_players[:self.repr_int])\n",
        "\n",
        "      # reproduction\n",
        "      while len(top_players) < self.total_population:           # l√≥gica de esto: priorizar que se mantenga el numero de jugadores\n",
        "        for player, score in list(top_players.items()):\n",
        "            if len(top_players) >= self.total_population:       # god forgive me for what i am about to code\n",
        "                break\n",
        "            cloned_player = copy.deepcopy(player)\n",
        "            top_players[cloned_player] = score        # creo que copiarlo con la misma score no es buena idea porque se sesga el ranking, pero con 0 da el mismo resultado, algo falla\n",
        "\n",
        "      return top_players\n",
        "\n",
        "\n",
        "\n",
        "    def count_strategies(self) -> dict[str, int]:\n",
        "        \"\"\"\n",
        "        Counts the number of played alive of each strategy, based on the\n",
        "        initial list of players. Should be computed analyzing the\n",
        "        'self.ranking' variable. Useful for the results plot/print (not needed\n",
        "        for the tournament itself)\n",
        "\n",
        "        Results:\n",
        "            - A dict, containing as values the name of the players and as\n",
        "         values the number of individuals they have now alive in the tournament\n",
        "        \"\"\"\n",
        "        strategy_count = {}\n",
        "\n",
        "        for player in self.ranking:\n",
        "            strategy_name = player.name\n",
        "            if strategy_name not in strategy_count:\n",
        "                strategy_count[strategy_name] = 1\n",
        "            else:\n",
        "                strategy_count[strategy_name] += 1\n",
        "\n",
        "        return strategy_count\n",
        "\n",
        "    def play(self, do_print: bool = False):\n",
        "        \"\"\"\n",
        "        Main call of the class. Performs the computations to simulate the\n",
        "        evolutionary tournament.\n",
        "\n",
        "        Parameters\n",
        "            - do_print (bool = False): if True, should print the ongoing\n",
        "            results at the end of each generation (i.e. print generation number,\n",
        "            and number of individuals playing each strategy).\n",
        "        \"\"\"\n",
        "\n",
        "        # initial population quantity for each player\n",
        "        count_evolution = {\n",
        "            player.name: [val] for player, val in zip(self.players, self.initial_population)\n",
        "        }\n",
        "\n",
        "        for generation in range(1, self.generations + 1):\n",
        "            # tournament for this generation\n",
        "            tournament = Tournament(self.players, n_rounds=self.n_rounds, error=self.error, repetitions=self.repetitions)\n",
        "            tournament.play()\n",
        "\n",
        "            # printing player count before evolving\n",
        "            current_count = self.count_strategies()\n",
        "            for player_name, count in current_count.items():\n",
        "                count_evolution[player_name].append(count)  # add count for current generation\n",
        "\n",
        "            # tournament result (accumulative between rounds)\n",
        "            result_tournament = {player: sum([score]) for player, score in tournament.ranking.items()}\n",
        "            self.ranking = result_tournament\n",
        "            self.ranking = self.natural_selection(result_tournament)\n",
        "\n",
        "            # print current results if do_print is True\n",
        "            if do_print:\n",
        "                print(f\"[üß¨] Generation {generation}:\")\n",
        "                for player_name, count in current_count.items():\n",
        "                    print(f\"{player_name}: {count} individuals\")\n",
        "                print(\"\\n\")\n",
        "        return count_evolution\n",
        "\n",
        "\n",
        "    # Si quieres obtener un buen gr√°fico de la evoluci√≥n, puedes usar este\n",
        "    # m√©todo si has seguido la pista indicada en la cabecera del m√©todo\n",
        "    # anterior. Ya est√° implementado, pero puede que necesites adaptarlo a tu\n",
        "    # c√≥digo.\n",
        "    def stackplot(self, count_evolution: dict[str, list]) -> None:\n",
        "        \"\"\"\n",
        "        Plots a 'stackplot' of the evolution of the tournament\n",
        "\n",
        "        Parameters:\n",
        "            - count_evolution (dict[Player, list]): a dictionary containing as\n",
        "         keys the name of the strategies of the different players of the\n",
        "         tournament. Each value is a list, where the 'i'-th position of that\n",
        "         list indicates the number of individuals that player has at the end of\n",
        "         the 'i'-th generation\n",
        "         \"\"\"\n",
        "\n",
        "        COLORS = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n",
        "\n",
        "        for i, name in enumerate(count_evolution.keys()):\n",
        "            plt.plot([], [], label=name, color= COLORS[(i) % len(COLORS)])\n",
        "\n",
        "        plt.stackplot(list(range(self.generations + 1)),\n",
        "                      np.array(list(count_evolution.values())), colors=COLORS)\n",
        "\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKHxXZEJDjzb"
      },
      "source": [
        "Para testear el m√≥dulo anterior, puedes usar alguno de los experimentos mostrados en el juego de Nicky Case. Por ejemplo, un torneo con 5 TFT, 5 Desertores y 15 Cooperantes, sin error, con 10 rondas por interacci√≥n, con una reproductividad del 0.2, parece que en menos de 10 generaciones converge a una poblaci√≥n dominada por TFT. Intenta replicar este resultado como se muestra a continuaci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWqS1N6SDjzb",
        "outputId": "0603e8e0-6e31-4a3c-974c-6757eb6291ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[üß¨] Generation 1:\n",
            "cooperator: 15 individuals\n",
            "defector: 5 individuals\n",
            "tft: 5 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 2:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 3:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 4:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 5:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 6:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 7:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 8:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 9:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[üß¨] Generation 10:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'cooperator': [15, 15, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
              " 'defector': [5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
              " 'tft': [5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8]}"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dilemma = Dilemma(2, -1, 3, 0)\n",
        "\n",
        "cooperator_player = Cooperator(dilemma, \"cooperator\")\n",
        "defector_player = Defector(dilemma, \"defector\")\n",
        "tft_player = Tft(dilemma, \"tft\")\n",
        "\n",
        "all_players = (cooperator_player, defector_player, tft_player)\n",
        "\n",
        "evolution = Evolution(all_players, n_rounds=10, error=0.00, repetitions=1,\n",
        "                      generations=10, reproductivity=0.2,\n",
        "                      initial_population=(15, 5, 5))\n",
        "\n",
        "evolution.play(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G4esDTCDjzc"
      },
      "source": [
        "¬°Enhorabuena! La primera parte de la pr√°ctica ya la has terminado. En la siguiente secci√≥n, tendr√°s que dise√±ar una estrategia para participar en un triple campeonato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U410RCwsDjzc"
      },
      "source": [
        "## Parte 2: dise√±o de una estrategia\n",
        "\n",
        "A continuaci√≥n deber√°s implementar una estrategia que se enfrentar√° a las de tus compa√±eros un un torneo. Antes de describir las condiciones del campeonato, vamos a ver un concepto nuevo.\n",
        "\n",
        "Los juegos contra tus rivales tendr√°n un final **no determinista**. Esto quiere decir que el n√∫mero de rondas que se van a jugar no se sabe con precisi√≥n. En lugar de eso, despu√©s de cada ronda, la partida tiene una cierta probabilidad de acabar (peque√±a). ¬øPor qu√© haremos esto? En verdad, el dilema del prisionero con una duraci√≥n finita es un problema completamente distinto, ya que la proximidad del final de la interacci√≥n juega un papel crucial en las decisiones de las estrategias.\n",
        "\n",
        "Con el objetivo de dejar esta complejidad adicional fuera, jugaremos un n√∫mero de rondas aleatorio: puedes pensar que tu estrategia va a jugar \"muchas veces\" contra cada rival. **En media, se jugar√°n 100 rondas** (puedes quedarte con ese n√∫mero). Por supuesto, para que los resultados no est√©n determinados por el n√∫mero de rondas jugadas, los puntos obtenidos en cada interacci√≥n se normalizar√°n por el n√∫mero de rondas jugadas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMVTH7ZWDjzc"
      },
      "source": [
        "### Descripci√≥n del campeonato\n",
        "Ahora s√≠, vamos a ver las condiciones del torneo. Hay tres fases en esta competici√≥n:\n",
        " - Fase de enfrentamiento directo entre estrategias\n",
        " - Fase evolutiva\n",
        " - Fase evolutiva dentro del ecosistema completo del DPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwTEllmEDjzc"
      },
      "source": [
        "\n",
        "#### Fase de enfrentamiento directo entre estrategias\n",
        "\n",
        "Se jugar√° un torneo de *todos contra todos*: te enfrentar√°s a las estrategias de tus rivales dos veces. El resultado que obtengas en cada enfrentamiento se sumar√° a los ya obtenidos hasta el momento. Ganar√° la estrategia con m√°s puntos al final de las interacciones. Las condiciones concretas son las siguientes:\n",
        " - El dilema del prisionero que se usar√° ser√° el siguiente:\n",
        "\n",
        "<center>\n",
        "\n",
        "|      |  C    |  D    |\n",
        "|------|-------|-------|\n",
        "|   C  | 2, 2  | -1, 3  |\n",
        "|   D  | 3, -1  | 0, 0  |\n",
        "\n",
        "</center>\n",
        "\n",
        " - La probabilidad de acabar el enfrentamiento tras cada ronda $P_{end}$ se fija en 1%, con un m√°ximo de rondas de 250.\n",
        " - La probabilidad de error $P_{error}$ se fija en 1%.\n",
        " - Se jugar√° 2 veces contra cada rival.\n",
        "\n",
        "Se asignar√°n los siguientes puntos seg√∫n la posici√≥n final:\n",
        " - 5¬∫ y 6¬∫ clasificados: 4 puntos\n",
        " - 4¬∫ clasificado: 8 puntos\n",
        " - 3¬∫ clasificado: 12 puntos\n",
        " - 2¬∫ clasificado: 17 puntos\n",
        " - 1¬∫ clasificado: 24 puntos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0oAmY4vDjzd"
      },
      "source": [
        "\n",
        "#### Fase evolutiva\n",
        "\n",
        "Las estrategias se enfrentar√°n en un torneo evolutivo, en el que todos parten con el mismo n√∫mero de representantes. Las condiciones para cada generaci√≥n son las mismas que en la fase anterior.\n",
        "\n",
        "La reproductividad se manejar√° de forma ligeramente distinta a lo visto en secciones anteriores. Ahora, en sucesivas generaciones, la proporci√≥n de individuos de un jugador frente al total ser√° la misma que la proporci√≥n de puntos obtenidos por todos esos individuos frente al total de puntos obtenidos por todas las estrategias. Por ejemplo, imagina que una determinada generaci√≥n tienes 17 individuos de un total de 100. Estos individuos, en suma han obtenido un total de 210 puntos. Adem√°s, la suma de puntos obtenidos por todas las individuos de la poblaci√≥n es 1000 puntos. Por tanto, en la siguiente generaci√≥n dispondr√°s de 21 individuos.\n",
        "\n",
        "Aqu√≠ la evaluaci√≥n de los ganadores puede ser complicada. En principio, el orden en el que se extingan las estrategias indicar√° el ranking de esta fase. No obstante, en ocasiones surgen situaciones dif√≠ciles de evaluar (por ejemplo, comportamientos c√≠clicos de dominancia de varias estrategias). En este tipo de escenarios, ser√°n los profesores quienes decidan qu√© estrategias han sido las m√°s exitosas y su orden. Los puntos que pueden obtenerse en esta fase son los siguientes:\n",
        " - 3¬∫ clasificado: 12 puntos\n",
        " - 2¬∫ clasificado: 17 puntos\n",
        " - 1¬∫ clasificado: 24 puntos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw1W28JNDjzd"
      },
      "source": [
        "#### Fase evolutiva dentro del ecosistema completo del DPI\n",
        "\n",
        "En esta fase, tu estrategia se enfrentar√° en un gran torneo evolutivo, donde estar√°n incluidas las estrategias de tus compa√±eros, pero tambi√©n hasta 50 estrategias adicionales del ecosistema del DPI. Estar√°n las que ya conoces (TFT, Grudger, etc.), y tambi√©n las estrategias que se han mostrado m√°s exitosas en las √∫ltimas publicaciones cient√≠ficas. De nuevo, el criterio general para establecer el ranking entre nuestras estrategias ser√°: \"cuanto m√°s tarde se extingan tus individuos, m√°s alto estar√°s en el ranking\". Al haber tantas estrategias involucradas, esta parte del campeonato es la m√°s susceptible a presentar situaciones dif√≠ciles de evaluar, por lo que en √∫ltima instancia ser√°n los profesores quienes, partiendo de criterios objetivos, seleccionen las tres estrategias m√°s exitosas, que se llevar√°n:\n",
        " - 3¬∫ clasificado: 4 puntos\n",
        " - 2¬∫ clasificado: 8 puntos\n",
        " - 1¬∫ clasificado: 12 puntos\n",
        "\n",
        "Los puntos de las tres fases se sumar√°n y **el grupo ganador obtendr√° 0.4 puntos extra en la evaluaci√≥n de la asignatura**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKDATcc9Djzd"
      },
      "source": [
        "### Implementaci√≥n de tu estrategia\n",
        "\n",
        "A la hora de implementar tu estrategia ten en cuenta lo siguiente:\n",
        " - Si has seguido la plantilla de desarrollo de la primera parte de la pr√°ctica, tu estrategia deber√° ser implementada como un subclase de la clase abstracta ```Player()```. En particular, debes implementar cuidadosamente su m√©todo ```strategy()```.\n",
        " - No olvides darle un buen nombre a tu estrategia.\n",
        " - Igualmente, mientras implementas, incluye todo los comentarios que puedan ayudar a los profesores a evaluar positivamente tu estrategia. Por ejemplo: \"*he visto que mi estrategia funcionaba mal contra Desertores, por eso incluyo el siguiente bloque de c√≥digo que pretende...*\".\n",
        " - **¬°IMPORTANTE!** Incluye informaci√≥n suficientemente extensa para explicar tus decisiones de dise√±o, por qu√© presentas esa estrategia y no otra, cu√°les son los resultados que has observado en las pruebas que has hecho, qu√© problemas te has encontrado y c√≥mo los has solucionado, etc.\n",
        "\n",
        "A continuaci√≥n se incluye un ejemplo de implementaci√≥n de una estrategia. De hecho, este ejemplo ser√° la estrategia de los profesores, ¬°y participar√° en el campeonato! As√≠ que ya sabes una de las estrategias que va a estar presente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "R5XM5SjTDjze"
      },
      "outputs": [],
      "source": [
        "class Destructomatic(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"\"):\n",
        "        \"\"\"...\"\"\"\n",
        "        super().__init__(dilemma, name)\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"\n",
        "        A \"random\" friend of a tf2t\n",
        "        Starts cooperating, very forgiving\n",
        "        It defects randomly ¬ø? =D\n",
        "        \"\"\"\n",
        "\n",
        "        turns = len(self.history)\n",
        "\n",
        "        if turns < 2:  # I don't want any trouble, my friend (at least now)\n",
        "            return C\n",
        "\n",
        "        # [I need some data to draw up my plan...] \"I'm very forgiving :)\"\n",
        "        if turns < 10:\n",
        "            if opponent.history[-2:] != [D, D]:\n",
        "                return C\n",
        "            else:\n",
        "                return D\n",
        "\n",
        "        count_D_opponent = opponent.history.count(D)  # Num. of defections of the opponent\n",
        "\n",
        "        # This is my rule, dude\n",
        "        if count_D_opponent / turns > random.random():\n",
        "            return D\n",
        "\n",
        "        # Ok, but in general I'm very forgiving, as you see\n",
        "        if opponent.history[-2:] != [D, D]:  # tf2t condition\n",
        "            return C\n",
        "\n",
        "        # Otherwise -> opponent.history[-2:] == [D, D]\n",
        "        return D\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YrHbGtdDjzf"
      },
      "source": [
        "#### Espacio para el desarrollo de tu estrategia\n",
        "\n",
        "Implementa tu estrategia en la plantilla de la siguiente celda, modific√°ndola a conveniencia. Puedes a√±adir m√°s m√©todos si lo necesitas. Incluye una descripci√≥n explicando el proceso que has seguido para dise√±arla: usa el docstring de tu clase o una celda de texto adicional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "U_uGTFc6Djzf"
      },
      "outputs": [],
      "source": [
        "class T10(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"T10\"):\n",
        "        \"\"\"T10\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "        self.point = 0\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Cooperates first, then use TfT if we are wining, else we change to Grudger or Win‚Äìstay, lose‚Äìswitch if we are losing\"\"\"\n",
        "\n",
        "        turn = len(self.history)             # Current Turn\n",
        "        count_D = opponent.history.count(D)  # Num. of defections of the oponent\n",
        "        count_C = opponent.history.count(C)  # Num. of cooperations of the oponent\n",
        "        #Point calculator\n",
        "        if len(self.history) > 0 and len(opponent.history) > 0:\n",
        "            self.point += pd.evaluate_result(self.history[-1], opponent.history[-1])[0]\n",
        "\n",
        "        #We start at turn 1 cooperating\n",
        "        if turn == 0 or turn == 1:\n",
        "            self.history.append(C)\n",
        "            return C  \n",
        "        \n",
        "        if turn == 9 and (opponent.history[-1] == 0 and opponent.history[-2] == 0 and opponent.history[-3] == 0):\n",
        "            self.history.append(D)\n",
        "            return D \n",
        "        \n",
        "        #If at mid round we are losing we use Grudger or Win‚Äìstay until end match\n",
        "        if 10 >= turn >= 5 and self.point <= 10:\n",
        "            self.point =0\n",
        "            if count_D > count_C:\n",
        "            #Grudger\n",
        "                action = C\n",
        "                if len(opponent.history) > 0 and len(self.history) > 0:\n",
        "                    if opponent.history[-1] == D or self.history[-1] == D:\n",
        "                        action = D\n",
        "                return action\n",
        "            \n",
        "            else:\n",
        "                # Win‚Äìstay, lose‚Äìswitch\n",
        "                last_round = self.history[-1], opponent.history[-1]\n",
        "                if last_round[0] == C and last_round[1] == D:\n",
        "                    self.history.append(D)\n",
        "                    return D # Switch before lose\n",
        "                else:\n",
        "                    self.history.append(C)\n",
        "                    return C # Stil cooperates before winning\n",
        "        \n",
        "        #Until that (if we are winning), we follow TfT\n",
        "        if 5 >= turn > 1 or (10 >= turn > 5 and self.point > 10):\n",
        "            opponents_last_action = opponent.history[-1]\n",
        "            self.history.append(opponents_last_action)\n",
        "            return opponents_last_action\n",
        "\n",
        "        else:\n",
        "            opponents_last_action = opponent.history[-1]\n",
        "            self.history.append(opponents_last_action)\n",
        "            return opponents_last_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku6fakGHDjzf"
      },
      "source": [
        "Para la entrega, explica en detalle el proceso de dise√±o de tu estrateg√≠a, por qu√© has decidido presentar esa estrategia y no otras con las que hayas probado, qu√© consideraciones has tenido en cuenta al dise√±arla, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selected strategy and proposed considerations ###\n",
        "\n",
        "##### Study of previous strategies #####\n",
        "\n",
        "Since the inception of the prisoner's dilemma many strategies have gained importance over the years along with a unique approach to each of them. According to Robert Axelrod in several of his investigations carried out from the study of many game strategies, the best ones shared a series of characteristics that made them have a higher final score, which we have taken into consideration for the elaboration of our new strategy. These features are the following ones\n",
        "\n",
        "- Promotion of Cooperation: Numerous contemporary strategies emphasize the optimization of cooperative interactions over defection, with the primary goal of avoiding initiation of defection by these strategies.\n",
        "\n",
        "- Retaliatory Mechanisms: Axelrod argued that an effective strategy should not exhibit unwavering optimism. It should consider the possibility that a defection could exploit strategies prioritizing collaboration over defection.\n",
        "\n",
        "- Forgiveness Dynamics: The incorporation of forgiveness into algorithms grants them the capacity to pardon an opponent for ceasing to defect. This dynamic fosters an environment conducive to mutual collaboration and maximizes overall payoffs.\n",
        "\n",
        "- Clarity Requirement: This attribute advises strategies to be easily predictable, promoting enhanced mutual cooperation even when uncertainties exist regarding the intentions of the opponent.\n",
        "\n",
        "As an example of strategies that employ some of these four conditions, the TFT strategy has been considered to be the most effective one using the \"Nice\", \"Retaliate\" and \"Forgive\" features.\n",
        "\n",
        "\n",
        "Numerous strategies proposed throughout the prisoner's dilemma are based on variations or combinations of other strategies, following a number of specific criteria. A notable example of this is the Grudger strategy, which transitions from a cooperative stance to a fully deserting one in the event that the opponent decides to defect on a single occasion.\n",
        "\n",
        "\n",
        "With this in mind we will aim to make our own strategy follow at least three of the four conditions for best performance.\n",
        "\n",
        "##### Human experimentation #####\n",
        "\n",
        "In order to draw conclusions and explore more effective approaches to determine an optimal strategy, we decided to recruit volunteers to repeat the prisoner's dilemma under the same rules proposed in practice. During this testing phase, we conducted ten rounds using the same point system, which led to a number of results:\n",
        "\n",
        "- Throughout the experiment, several participants opted for a defection strategy. Even despite the persistence of this strategy, many showed a willingness to \"forgive\" spontaneously, despite repeatedly disloyal behavior on the part of the opponent.\n",
        "\n",
        "- From the middle of the experiment (either turn 5 or 6), participants decided to change their strategies based on their previous learning, opting for completely different strategies.\n",
        "\n",
        "- In situations where both players cooperated in one turn, many of them chose to defect in the next two turns in order to score more points, resulting in several rounds of defection on both sides.\n",
        "\n",
        "With these observations in mind, we have decided to incorporate several of the behaviors identified during the test and fuse them into a strategy that integrates some of these principles. The goal is to develop our own unique strategy capable of rivaling some of the most commonly used strategies in the dilemma.\n",
        "\n",
        "### Description of the strategy ###\n",
        "\n",
        "The T1O strategy in the Prisoner's Dilemma is a dynamic approach that evolves throughout the game. Initially cooperative, the player employs a Tit for Tat strategy for several turns, reflecting the last action of the opponent. As the game evolves, the strategy adapts according to the accumulated points and historical actions. If the points exceed a threshold or the game reaches a certain turn, the player continues with TFT. However, if the points are below a specified level, the strategy can change to continued defection or employ a \"Win‚Äìstay, lose‚Äìswitch\" tactic, defecting only if the opponent defected in the previous round. This blended strategy combines cooperative and retaliatory elements, showing a strategic response to diverse game conditions.\n",
        "\n",
        "For last, we have a secret condition due to the experienced acquired in experimentation, in the final round if we have cooperated three times in the past turns, we will defect no matter what, demonstrating efficiency against \"nice\" strategies.\n",
        "\n",
        "-----\n",
        "\n",
        "And why we change to WSLS and ADFL?\n",
        "\n",
        "According to Robert Axelrod in Evolution of Cooperation, \"If the game is played a known finite number of times, the players still have no incentive to cooperate.\" meaning that however the strategy we have, it is impossible to determine one hundred percent that the opponent will cooperate, either because of the existence of error, past patterns or predictable behaviors. Considering the consistent pattern of defections observed early in the rounds, we believe the most viable strategy is to defect in all subsequent rounds.\n",
        "\n",
        "### References ###\n",
        "\n",
        "Nowak M, Sigmund K. A strategy of win-stay, lose-shift that outperforms tit-for-tat in the Prisoner's Dilemma game. Nature. 1993 Jul 1;364(6432):56-8.\n",
        "\n",
        "Milinski M. Evolutionary biology. Cooperation wins and stays. Nature. 1993 Jul 1;364(6432):12-3.\n",
        "\n",
        "82 S. Cal. L. Rev. 209 (2008-2009) Beyond the Prisoners' Dilemma: Coordination, Game Theory, and Law \n",
        "\n",
        "Axelrod, R. 1984. The Evolution of Cooperation. New York: Basic Books.\n",
        "\n",
        "Rapoport, A. and Chammah, A.M. 1965. Prisoner‚Äôs Dilemma. Ann Arbor: University of Michigan Press.\n",
        "\n",
        "Nowak, M.; Sigmund, K. (July 1, 1993). \"A strategy of win-stay, lose-shift that outperforms tit-for-tat in the Prisoner's Dilemma game\". Nature. 364 (6432): 56‚Äì58\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "cgBX2mKqDjzg"
      },
      "outputs": [],
      "source": [
        "# Haz todas las pruebas que necesites aqu√≠.\n",
        "# Por ejemplo:\n",
        "#\n",
        "# game = Game(Destructomatic(dilemma, \"destr\"), Tft(dilemma, \"tft\"),\n",
        "#             dilemma, n_rounds=10, error=0.1)\n",
        "# game.play(do_print=True)\n",
        "#\n",
        "# O tambi√©n:\n",
        "#\n",
        "# dilemma = Dilemma(13, 0, 20, 4)\n",
        "# participants = (Destructomatic(dilemma, \"destr\"),\n",
        "#                 Cooperator(dilemma, \"coop1\"),\n",
        "#                 Defector(dilemma, \"defect\"),\n",
        "#                 Cooperator(dilemma, \"coop2\"),\n",
        "#                 Tft(dilemma, \"tft\"),\n",
        "#                 Detective4MovsTFT(dilemma, \"detect\"))\n",
        "#\n",
        "# tournament = Tournament(participants, dilemma, n_rounds=100, error=0.01,\n",
        "#                         repetitions=2)\n",
        "# tournament.play()\n",
        "# tournament.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjN-2JJ8Djzg"
      },
      "source": [
        "### R√∫brica de calificaci√≥n de la pr√°ctica\n",
        "\n",
        "PARTE 1:\n",
        " - **[4/10] Funcionamiento del c√≥digo**: el c√≥digo descrito en la *Parte 1* de la pr√°ctica debe funcionar correctamente, sin *bugs*, permitiendo reproducir todas las funcionalidades descritas.\n",
        " - **[2/10] Limpieza y claridad de estilo**.\n",
        "  \n",
        "PARTE 2:\n",
        " - **[1/10] Implementaci√≥n de una estrategia correcta**: que funcione sin errores y que est√© programada con claridad.\n",
        " - **[3/10] Originalidad de la estrategia y trabajo de exploraci√≥n llevado a cabo**: el tipo de estrategia presentada, junto con las explicaciones que hayas aportado en comentarios y dem√°s, pondr√° de manifiesto el trabajo de experimentaci√≥n que has llevado a cabo con tu estrategia. Este item pretende evaluar este aspecto.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LEXbkD7Djzg"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1beef6bdf7cb3edc825034b485381c9781f3bd7bbda8256bc912318d884443a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
