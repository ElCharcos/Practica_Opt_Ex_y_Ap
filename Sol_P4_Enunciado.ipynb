{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-lbjokgDjzD"
      },
      "source": [
        "# Práctica 1 - Dilema del Prisionero Iterado\n",
        "\n",
        "## Introducción\n",
        "\n",
        "La *Teoría de Juegos* es una rama de las matemáticas que estudia la interacción entre individuos racionales. Para su estudio, es habitual que se utilicen *juegos* o *dilemas* en los que los agentes implicados puedan escoger entre varias *acciones*. El resultado del juego dependerá de la acción propia y las de los oponentes. Un ejemplo muy conocido es el juego de *piedra, papel y tijera*.\n",
        "\n",
        "Probablemente el *juego* o *dilema* más estudiado dentro de la Teoría de Juegos es el [**Dilema del Prisionero**](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma). En este dilema, los jugadores tienen dos opciones: cooperar (C) o desertar (D). A la siguiente matriz se la conoce como *matriz de pagos* (*payoff matrix*), y es una forma compacta de representar un juego: por filas se representa a un jugador, y por columnas al otro.\n",
        "\n",
        "<center>\n",
        "\n",
        "|      |  C    |  D    |\n",
        "|------|-------|-------|\n",
        "|   **C**  | 2, 2  | -1, 3 |\n",
        "|   **D**  | 3, -1 |  0, 0 |\n",
        "\n",
        "</center>\n",
        "\n",
        "Las celdas interiores de la matriz establecen los pagos que se lleva cada jugador. El primer número de la tupla es el pago al jugador *fila*, y el segundo número el pago al jugador *columna*. Por ejemplo, si el jugador *fila* deserta (D) y el jugador *columna* coopera (C), el primero se llevará 3 puntos y el segundo *-1*. En verdad, los valores de la tabla no son *puntos*, es una medida sin dimensiones cuya única función es permitirnos comparar resultados. De hecho, no existe un *único* dilema del prisionero: se pueden modificar los valores anteriores libremente, siempre que se mantengan las relaciones ordinales entre ellos.\n",
        "\n",
        "Para juegos simétricos como el dilema del prisionero, suele ser más conveniente utilizar la *matriz de pagos simplificada*\n",
        "\n",
        "<center>\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "2 & -1 \\\\\n",
        "3 & 0\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "No olvides que los jugadores del dilema del prisionero son jugadores racionales. Además, **el objetivo no es *ganar* al rival, sino maximizar el beneficio personal**. Estas dos premisas son las hipótesis sobre las que se sustenta todo lo que viene a continuación.\n",
        "\n",
        "Cuando se juega al dilema del prisionero una única vez, la estrategia óptima es la de desertar (ejercicio: reflexiona sobre el por qué). Sin embargo, la complejidad surge cuando los jugadores se enfrentan repetidamente con el mismo oponente, ya que sus acciones actuales influirán en su reputación a largo plazo. Esta variante se conoce como el **Dilema del Prisionero Iterado** y ha sido objeto de profundo estudio en la literatura científica. A pesar de que John von Neumann fue el primero en plantear este dilema, Robert Axelrod ha destacado como uno de los científicos más influyentes en su análisis.\n",
        "\n",
        "Axelrod planteó varios \"campeonatos computacionales\", en los que los participantes debían proponer una estrategia para jugar al DPI. Luego Axelrod enfrentó todas las estrategias y vio cuáles eran las que mejor funcionaban. Una *estrategia* para jugar al DPI es un conjunto de instrucciones inequívocas que, a partir del histórico de movimientos de ambos jugadores, dice cuál debe ser el siguiente movimiento. Por ejemplo, lo siguiente sería una estrategia: \"comienzo cooperando, luego deserto dos veces, y a partir de la cuarta ronda coopero siempre\". Algunas de estas estrategias se han hecho famosas por ser exitosas o por aparece con frecuencia en la literatura.\n",
        "\n",
        "Axelrod comenzó estudiando torneos *normales* (llamados *de enfrentamiento directo*), donde cada participante (estrategia) es enfrentado contra cada uno de los demás, sumando los resultados obtenidos en cada interacción, y estableciendo un ranking al final del campeonato. También estudió los llamados *torneos evolutivos*. En ellos cada estrategia parte con un número de individuos representantes que juegan siempre esa estrategia. Tras enfrentar todos los individuos contra todos, se elimina de la población los que peor resultado han obtenido, sustituyéndolos por los más exitosos. Este proceso se repite varias veces, dando lugar (en muchos casos) a que una estrategia coloniza a la población completa: se encuentra la estrategia óptima. En escenarios más avanzados, se permite también la evolución de las estrategias a lo largo de las generaciones, mediante mecanismos de mutación y cruce (selección natural).\n",
        "\n",
        "Las metaheurísticas son técnicas de optimización confeccionadas para casos particulares, complejos y dinámicos. El DPI evolutivo es ejemplo tangible de esta técnica. En este problema, múltiples soluciones compiten y evolucionan hacia la optimización de un objetivo dado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3IuFnKvDjzI"
      },
      "source": [
        "## Descripción de la práctica\n",
        "\n",
        "**¡Importante!** Comienza jugando al juego de Nicky Case llamado *The evolution of trust* (~30min). Es un buen resumen de los conceptos previos que necesitas saber sobre el DPI antes de comenzar la práctica:\n",
        "<center>\n",
        "\n",
        "[<img src=\"https://ncase.me/img/trust.png\" alt=\"The evolution of trust: https://ncase.me/trust/\" width=\"400\">](https://ncase.me/trust/)\n",
        "\n",
        "https://ncase.me/trust/\n",
        "</center>\n",
        "Unas aclaraciones sobre el juego: en ocasiones, el autor utiliza una nomenclatura que no es habitual en la literatura del DPI. Por evitar confusiones:\n",
        "\n",
        " - Las acciones posibles en el dilema las llamaremos Cooperar (*Cooperate*) y Desertar (*Defect*), y no *cheat*.\n",
        " - La estrategia que repite el último movimiento del rival se conoce ampliamente como *Tit-For-Tat*, y no *Copycat*.\n",
        "\n",
        "La práctica tiene dos partes:\n",
        " - **Parte 1**: Montar la estructura computacional que permita simular torneos del DPI.\n",
        " - **Parte 2**: Diseñar una estrategia, que se enfrentará a las de tus compañeros en un torneo de tres fases descrito a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93D6PBnyDjzJ"
      },
      "source": [
        "## Parte 1: Sofware para el estudio del DPI\n",
        "\n",
        "El primer objetivo es crear un sofware que permita simular torneos del DPI. En concreto, el software debe cumplir los siguientes requisitos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUxRfYmHDjzK"
      },
      "source": [
        " - Para el torneo de enfrentamiento directo, las entradas al programa deben ser:\n",
        "   - ```all_players```: lista o tupla de jugadores que van a participar en el campeonato (con información sobre sus estrategias).\n",
        "   - ```dilemma```: objeto para definir el dilema del prisionero jugado (en particular, deberá contener la información de la correspondiente matriz de pagos, que es la mostrada arriba).\n",
        "   - ```n_rounds```: número de rondas por cada enfrentamiento.\n",
        "   - ```error```: probabilidad de error por jugada.\n",
        "   - ```repetitions```: el número de veces que una estrategia va a enfrentarse a otra.\n",
        " - La salida debe ser la información (visual) sobre el resultado del campeonato.\n",
        " - Además, el programa deberá contener al menos las 5 estrategias básicas descritas en el juego de Nicky Case. Igualmente, deberá permitir añadir nuevas estrategias de forma sencilla.\n",
        "\n",
        "Consulta la sección sobre el *módulo evolutivo* para ver las particularidades de esa parte.\n",
        "\n",
        "A continuación se propone una **plantilla de desarrollo** con los módulos/clases que se recomienda implementar para resolver este problema. No es obligatorio seguir esta estructura. Por otro lado, puede ser recomendable organizar el código en un proyecto local en lugar de en *Jupyter* o *Colab*. Si quieres hacerlo, la estructura del proyecto recomendada es la siguiente:\n",
        "\n",
        "```\n",
        " DPI/\n",
        "   ├── dpi/\n",
        "   │   ├── __init__.py\n",
        "   │   ├── dilemma.py  # implementa la lógica del DP jugado, acciones posibles, matriz de pagos, etc\n",
        "   │   ├── player.py  # implementa la definición de \"Jugador\" y en particular, la estrategia\n",
        "   │   ├── game.py  # implementa la dinámica de un juego cara a cara entre dos jugadores\n",
        "   │   ├── tournament.py  # implementa la dinámica del torneo general\n",
        "   │   └── evolution.py  # implementa la dinámica del torneo evolutivo\n",
        "   ├── main.py # main del proyecto - si quieres, usa varios 'mains' para testear los distintos módulos\n",
        "   └──...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXaTlPU4DjzM"
      },
      "source": [
        "### Plantilla de desarrollo\n",
        "\n",
        "Se incluye a continuación una plantilla con los módulos/clases que se recomienda implementar, junto con variables y métodos necesarios. El objetivo es facilitar la tarea de diseñar el código. No es obligatorio seguir esta estructura, pero a falta de mejores ideas, puede ser un buen punto de partida.\n",
        "\n",
        "Para facilitar la tarea de identificar qué métodos faltan por implementar, en todos ellos se ha incluido la excepción ```raise NotImplementedError``` para poner de manifiesto que dicho método debe ser implementado.\n",
        "\n",
        "Se comienza importando librerías. El uso de otras librerías **basicas** también está permitido, siempre que esté justificado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\charly\\anaconda3\\lib\\site-packages (1.24.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\charly\\anaconda3\\lib\\site-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\charly\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "jOQdcOwWDjzO"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import typing\n",
        "import itertools\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w41RjVoMDjzP"
      },
      "source": [
        "#### El módulo ```dilemma```\n",
        "\n",
        "El primer módulo que se recomienda implementar es el que recoge la información sobre el dilema jugado. Como el DP solo tiene dos acciones posibles (*Cooperate* y *Defect*), por simplicidad crearemos dos variables globales ```C``` y ```D``` asociadas a dos enteros (0 y 1 respectivamente). Representarán estas dos acciones a lo largo de todo el código:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "mvUPibN8DjzQ"
      },
      "outputs": [],
      "source": [
        "C = 0\n",
        "D = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udrWfLuQDjzR"
      },
      "source": [
        "Ahora se propone implementar una clase llamada ```Dilemma``` para representar dilemas simétricos 2x2, como el dilema del prisionero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "9esWjcgnDjzS"
      },
      "outputs": [],
      "source": [
        "class Dilemma:\n",
        "\n",
        "    def __init__(self, cc: float, cd: float, dc: float, dd: float):\n",
        "        \"\"\"\n",
        "        Represents a 2x2 symmetric dilemma.\n",
        "\n",
        "        Parameters:\n",
        "            - cc (float): payoff for mutual cooperation\n",
        "            - cd (float): payoff when one cooperates, but the opponent defects\n",
        "            - dc (float): payoff when one defects, and the opponent cooperates\n",
        "            - dd (float): payoff for mutual defection\n",
        "        \"\"\"\n",
        "        self.dilema_matrix = np.array([np.array([cc, cd]), np.array([dc, dd])])\n",
        "\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def payoff_matrix(self) -> npt.NDArray[np.floating]:\n",
        "        \"\"\"\n",
        "        Symmetric pay-off matrix of the dilema\n",
        "\n",
        "        Returns:\n",
        "            - 2x2 np array of the matrix\n",
        "        \"\"\"\n",
        "        return self.dilema_matrix\n",
        "\n",
        "    @abstractmethod\n",
        "    def evaluate_result(self, a_1: int, a_2: int) -> tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Given two actions, returns the payoffs of the two players.\n",
        "\n",
        "        Parameters:\n",
        "            - a_1 (int): action of player 1 ('C' or 'D', i.e. '1' or '0')\n",
        "            - a_2 (int): action of player 2 ('C' or 'D', i.e. '1' or '0')\n",
        "\n",
        "        Returns:\n",
        "            - tuple of two floats, being the first and second values the payoff\n",
        "            for the first and second player, respectively.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.dilema_matrix[a_1, a_2], self.dilema_matrix[a_2, a_1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjwkGv5xDjzT"
      },
      "source": [
        "Comprueba que los métodos anteriores funcionan correctamente con el siguiente test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvQe2DeiDjzU",
        "outputId": "6b231a03-6e06-4dcc-ad1d-25af301b772f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, 0) -> (2, 2)\n",
            "(0, 1) -> (-1, 3)\n",
            "(1, 0) -> (3, -1)\n",
            "(1, 1) -> (0, 0)\n"
          ]
        }
      ],
      "source": [
        "#Prints all possible outcomes of the PD\n",
        "pd = Dilemma(2, -1, 3, 0)\n",
        "posible_actions = (C, D)\n",
        "for a1, a2 in itertools.product(posible_actions, repeat=2):\n",
        "    print(f\"{(a1, a2)} -> {pd.evaluate_result(a1, a2)}\")\n",
        "\n",
        "# Output:\n",
        "# (0, 0) -> (2, 2)\n",
        "# (0, 1) -> (-1, 3)\n",
        "# (1, 0) -> (3, -1)\n",
        "# (1, 1) -> (0, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSXC4GF_DjzU"
      },
      "source": [
        "#### El módulo ```player```\n",
        "\n",
        "Ahora vamos a programar una módulo que sirva para representar a los jugadores. En particular, su método principal será ```strategy()```, que devolverá una acción a realizar, en base a la historia de interacción con otro jugador. Además, aprovecharemos a implementar los *jugadores ilustres*, aquellos que juegan estrategias famosas, para que luego sea sencillo hacer pruebas.\n",
        "\n",
        "Empezaremos programando una clase abstracta llamada ```player``` de la que heredarán los jugadores concretos que programaremos a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "qY4GXEjnDjzV"
      },
      "outputs": [],
      "source": [
        "class Player(ABC):\n",
        "\n",
        "    # Este método ya está implementado\n",
        "    @abstractmethod\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"\"):\n",
        "        \"\"\"\n",
        "        Abstract class that represents a generic player\n",
        "\n",
        "        Parameters:\n",
        "            - name (str): the name of the strategy\n",
        "            - dilemma (Dilemma): the dilemma that this player will play\n",
        "        \"\"\"\n",
        "\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "\n",
        "        self.history  = []  # This is the main variable of this class. It is\n",
        "                            # intended to store all the history of actions\n",
        "                            # performed by this player.\n",
        "                            # Example: [C, C, D, D, D] <- So far, the\n",
        "                            # interaction lasts five rounds. In the first one,\n",
        "                            # this player cooperated. In the second, he also\n",
        "                            # cooperated. In the third, he defected. Etc.\n",
        "\n",
        "    def __str__(self):\n",
        "       return self.name\n",
        "\n",
        "    # Este método ya está implementado\n",
        "    @abstractmethod\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"\n",
        "        Main call of the class. Gives the action for the following round of the\n",
        "        interaction, based on the history\n",
        "\n",
        "        Parameters:\n",
        "            - opponent (Player): is another instance of Player.\n",
        "\n",
        "        Results:\n",
        "            - An integer representing Cooperation (C=0) or Defection (D=1)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "    def compute_scores(self, opponent: Player) -> tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Compute the scores for a given opponent\n",
        "\n",
        "        Parameters:\n",
        "            - opponent (Player): is another instance of Player.\n",
        "\n",
        "        Results:\n",
        "            - A tuple of two floats, where the first value is the current\n",
        "            player's payoff, and the second value is the opponent's payoff.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # Este método ya está implementado\n",
        "    def clean_history(self):\n",
        "        \"\"\"Resets the history of the current player\"\"\"\n",
        "        self.history = []\n",
        "\n",
        "\n",
        "# A continuación se representan las 5 estrategias básicas del juego de Nicky Case\n",
        "\n",
        "class Cooperator(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"Smooth Cooperator\"):\n",
        "        \"\"\"Cooperator\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Cooperates always\"\"\"\n",
        "        self.history.append(C)\n",
        "        return C\n",
        "\n",
        "\n",
        "\n",
        "class Defector(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"Defector\"):\n",
        "        \"\"\"Defector\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Defects always\"\"\"\n",
        "        self.history.append(D)\n",
        "        return D\n",
        "\n",
        "\n",
        "class Tft(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"Tit-For-Tat\"):\n",
        "        \"\"\"Tit-for-tat\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Cooperates first, then repeat last action of the opponent\"\"\"\n",
        "\n",
        "        if len(opponent.history) > 0:\n",
        "          # repeat last action of the opponent\n",
        "          opponents_last_action = opponent.history[-1]\n",
        "          self.history.append(opponents_last_action)\n",
        "          return opponents_last_action\n",
        "        else:\n",
        "          self.history.append(C)\n",
        "          return C\n",
        "\n",
        "class Grudger(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"Grudger\"):\n",
        "        \"\"\"Grudger\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"\n",
        "        Cooperates always, but if opponent ever defects, it will defect for the\n",
        "        rest of the game\n",
        "        \"\"\"\n",
        "        # cooperates by default\n",
        "        action = C\n",
        "\n",
        "        # upon deflection, starts deflecting, checks its own history to\n",
        "        # maintain decision\n",
        "        if len(opponent.history) > 0 and len(self.history) > 0:\n",
        "          if opponent.history[-1] == D or self.history[-1] == D:\n",
        "            action = D\n",
        "            \n",
        "        self.history.append(action)\n",
        "        return action\n",
        "\n",
        "class Detective4MovsTft(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"4M TFT Detective\"):\n",
        "        \"\"\"Four movement - tit for tat detective\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"\n",
        "        Starts with a fixed sequence of actions: [C,D,C,C]. After that, if\n",
        "        the opponent has ever defected, plays 'TFT'. If not, plays 'Defector'.\n",
        "        \"\"\"\n",
        "        match len(self.history):\n",
        "          case 0:\n",
        "            result = C\n",
        "          case 1:\n",
        "            result = D\n",
        "          case 2:\n",
        "            result = C\n",
        "          case 3:\n",
        "            result = C\n",
        "          case _:\n",
        "            # On tournament, when Detective has history but\n",
        "            # opponent does not have because it has not played\n",
        "            # an error is thrown. With this, we get rid of\n",
        "            # the error and select the \"C\" which is the \n",
        "            # first option for detective\n",
        "            if len(opponent.history) == 0:\n",
        "              result = C\n",
        "            else:\n",
        "              result = opponent.history[-1]\n",
        "\n",
        "        self.history.append(result)\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [],
      "source": [
        "class T10(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"T10\"):\n",
        "        \"\"\"T10\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "        self.point = 0\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Cooperates first, then use TfT if we are wining, else we change to Grudger or Win–stay, lose–switch if we are losing\"\"\"\n",
        "\n",
        "        turn = len(self.history)             # Current Turn\n",
        "        count_D = opponent.history.count(D)  # Num. of defections of the oponent\n",
        "        count_C = opponent.history.count(C)  # Num. of cooperations of the oponent\n",
        "        #Point calculator\n",
        "        if len(self.history) > 0 and len(opponent.history) > 0:\n",
        "            self.point += pd.evaluate_result(self.history[-1], opponent.history[-1])[0]\n",
        "\n",
        "        #We start at turn 1 cooperating\n",
        "        if turn == 0 or turn == 1:\n",
        "            self.history.append(C)\n",
        "            return C  \n",
        "        \n",
        "        if turn == 10 and (opponent.history[-1] == 0 and opponent.history[-2] == 0 and opponent.history[-3] == 0):\n",
        "            self.history.append(D)\n",
        "            return D \n",
        "        \n",
        "        #If at mid round we are losing we use Grudger or Win–stay until end match\n",
        "        if 10 >= turn >= 5 and self.point <= 10:\n",
        "            self.point =0\n",
        "            if count_D > count_C:\n",
        "            #Grudger\n",
        "                action = C\n",
        "                if len(opponent.history) > 0 and len(self.history) > 0:\n",
        "                    if opponent.history[-1] == D or self.history[-1] == D:\n",
        "                        action = D\n",
        "                return action\n",
        "            \n",
        "            else:\n",
        "                # Win–stay, lose–switch\n",
        "                last_round = self.history[-1], opponent.history[-1]\n",
        "                if last_round[0] == C and last_round[1] == D:\n",
        "                    self.history.append(D)\n",
        "                    return D # Switch before lose\n",
        "                else:\n",
        "                    self.history.append(C)\n",
        "                    return C # Stil cooperates before winning\n",
        "        \n",
        "        #Until that (if we are winning), we follow TfT\n",
        "        if 5 >= turn > 1 or (10 >= turn > 5 and self.point > 10):\n",
        "            opponents_last_action = opponent.history[-1]\n",
        "            self.history.append(opponents_last_action)\n",
        "            return opponents_last_action\n",
        "\n",
        "        else:\n",
        "            opponents_last_action = opponent.history[-1]\n",
        "            self.history.append(opponents_last_action)\n",
        "            return opponents_last_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4qR0bgwDjzW"
      },
      "source": [
        "Testearemos este módulo una vez programemos el siguiente\n",
        "\n",
        "#### El módulo ```game```\n",
        "\n",
        "Ya sabemos manejar jugadores. Ahora vamos a enfrentarles. El presente módulo pretende recoger las herramientas necesarias para enfrentar a dos jugadores en una *partida* del dilema del prisionero iterado. Está compuesto por una clase llamada ```Game``` que implementa el método principal ```play()```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "monHjo20DjzW"
      },
      "outputs": [],
      "source": [
        "class Game:\n",
        "\n",
        "    # Este método ya está implementado\n",
        "    def __init__(self, player_1: Player,\n",
        "                       player_2: Player,\n",
        "                       n_rounds: int = 100,\n",
        "                       error: float = 0.0):\n",
        "        \"\"\"\n",
        "        Game class to represent an iterative dilema\n",
        "\n",
        "        Parameters:\n",
        "            - player_1 (Player): first player of the game\n",
        "            - player_2 (Player): second player of the game\n",
        "            - n_rounds (int = 100): number of rounds in the game\n",
        "            - error (float = 0.0): error probability (in base 1)\n",
        "        \"\"\"\n",
        "\n",
        "        assert n_rounds > 0, \"'n_rounds' should be greater than 0\"\n",
        "\n",
        "        self.player_1 = player_1\n",
        "        self.player_2 = player_2\n",
        "        self.n_rounds = n_rounds\n",
        "        self.error = error\n",
        "\n",
        "        self.score = (0.0, 0.0)  # this variable will store the final result of\n",
        "                                 # the game, once the 'play()' function has\n",
        "                                 # been called. The two values of the tuple\n",
        "                                 # correspond to the points scored by the first\n",
        "                                 # and second player, respectively.\n",
        "\n",
        "\n",
        "    def play(self, do_print: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Main call of the class. Play the game.\n",
        "        Stores the final result in 'self.score'\n",
        "\n",
        "        Parameters\n",
        "            - do_print (bool = False): if True, should print the ongoing\n",
        "            results at the end of each round (i.e. print round number, last\n",
        "            actions of both players and ongoing score).\n",
        "        \"\"\"\n",
        "        for round in range(1, self.n_rounds+1):\n",
        "          player1_move = self.player_1.strategy(self.player_2)\n",
        "          player2_move = self.player_2.strategy(self.player_1)\n",
        "          round_result = pd.evaluate_result(player1_move, player2_move)\n",
        "          self.score = tuple(map(sum, zip(self.score, round_result)))\n",
        "\n",
        "          if do_print == True:\n",
        "            print(f'===== ROUND {round} =====')\n",
        "            print(f'[😇] Player 1 ({str(self.player_1)}) decided: {player1_move}')\n",
        "            print(f'[😈] Player 2 ({str(self.player_2)}) decided: {player2_move}')\n",
        "            print(f'[📋] Round result: {round_result}')\n",
        "            print(f'[📅] Current score: {self.score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJATrCGwDjzX"
      },
      "source": [
        "Comprueba que los últimos dos módulos funcionan como se espera en la siguiente celda. Experimenta con distintas combinaciones de jugadores y observa que los resultados son los esperados. Por ejemplo, un Cooperador y un Grudger se pasan toda su interacción cooperando, por lo que en un juego de 10 rondas sin error, ambos deben obtener 20 puntos al final. Comprueba también que el error se está tratando como corresponde. Por ejemplo, fíjalo en un valor alto (e.g. 0.2) y comprueba que en el enfrentamiento de un Cooperador con un Grudger alguno deserta en alguna ocasión. Comprueba también que puedes enfrentar dos jugadores que jueguen la misma estrategia (simplemente crea dos instancias del mismo jugador, con distintos nombres)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XfwcYqIDjzX",
        "outputId": "ee30dc60-7f4b-46a5-8b29-55e5c1a2da91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== ROUND 1 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (2.0, 2.0)\n",
            "===== ROUND 2 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (4.0, 4.0)\n",
            "===== ROUND 3 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (6.0, 6.0)\n",
            "===== ROUND 4 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (8.0, 8.0)\n",
            "===== ROUND 5 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (10.0, 10.0)\n",
            "===== ROUND 6 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (12.0, 12.0)\n",
            "===== ROUND 7 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (14.0, 14.0)\n",
            "===== ROUND 8 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (16.0, 16.0)\n",
            "===== ROUND 9 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (18.0, 18.0)\n",
            "===== ROUND 10 =====\n",
            "[😇] Player 1 (T10) decided: 0\n",
            "[😈] Player 2 (cooperator) decided: 0\n",
            "[📋] Round result: (2, 2)\n",
            "[📅] Current score: (20.0, 20.0)\n"
          ]
        }
      ],
      "source": [
        "dilemma = Dilemma(2, -1, 3, 0)\n",
        "\n",
        "cooperator_player = Cooperator(dilemma, \"cooperator\")\n",
        "defector_player = Defector(dilemma, \"defector\")\n",
        "tft_player = Tft(dilemma, \"tft\")\n",
        "grudger_player = Grudger(dilemma, \"grudger\")\n",
        "detective_player = Detective4MovsTft(dilemma, \"detective\")\n",
        "el_t10 = T10(dilemma, \"T10\")\n",
        "\n",
        "#Modifica las siguientes líneas a conveniencia para llevar a cabo distintos tests\n",
        "game = Game(el_t10, cooperator_player, n_rounds=10, error=0.2)\n",
        "game.play(do_print=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBb8_qsjDjzY"
      },
      "source": [
        "#### El módulo ```torunament```\n",
        "\n",
        "Finalmente, llegamos al módulo que nos va a permitir simular el campeonato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "OgivyVhwDjzY"
      },
      "outputs": [],
      "source": [
        "class Tournament:\n",
        "\n",
        "    # Este método ya está implementado\n",
        "    def __init__(self, players: tuple[Player, ...],\n",
        "                       n_rounds: int = 100,\n",
        "                       error: float = 0.0,\n",
        "                       repetitions: int = 2):\n",
        "        \"\"\"\n",
        "        All-against-all tournament\n",
        "\n",
        "        Parameters:\n",
        "            - players (tuple[Player, ...]): tuple of players that will play the\n",
        "         tournament\n",
        "            - n_rounds (int = 100): number of rounds in the game\n",
        "            - error (float = 0.0): error probability (in base 1)\n",
        "            - repetitions (int = 2): number of games each player plays against\n",
        "         the rest\n",
        "        \"\"\"\n",
        "\n",
        "        self.players = players\n",
        "        self.n_rounds = n_rounds\n",
        "        self.error = error\n",
        "        self.repetitions = repetitions\n",
        "\n",
        "        # This is a key variable of the class. It is intended to store the\n",
        "        # ongoing ranking of the tournament. It is a dictionary whose keys are\n",
        "        # the players in the tournament, and its corresponding values are the\n",
        "        # points obtained in their interactions with each other. In the end, to\n",
        "        # see the winner, it will be enough to sort this dictionary by the\n",
        "        # values.\n",
        "        self.ranking = {player: 0.0 for player in self.players}  # initial vals\n",
        "\n",
        "\n",
        "    def sort_ranking(self) -> None:\n",
        "        \"\"\"Sort the ranking by the value (score)\"\"\"\n",
        "        # aqui prefiero castear todo tras comprobar el isinstance pero bueno parche rapido\n",
        "        self.ranking = dict(sorted(self.ranking.items(), key=lambda item: item[1] if isinstance(item[1], (int, float)) else 0, reverse=True))\n",
        "\n",
        "    #pista: utiliza 'itertools.combinations' para hacer los cruces\n",
        "    def play(self) -> None:\n",
        "        \"\"\"\n",
        "        Main call of the class. It must simulate the championship and update\n",
        "        the variable 'self.ranking' with the accumulated points obtained by\n",
        "        each player in their interactions.\n",
        "        \"\"\"\n",
        "        for player1, player2 in itertools.combinations(self.players, 2):\n",
        "            game = Game(player1, player2, self.n_rounds, self.error)\n",
        "            for _ in range(self.repetitions):\n",
        "                game.play()\n",
        "                self.update_ranking(player1, player2, game.score)\n",
        "\n",
        "    def update_ranking(self, player1, player2, game_score):\n",
        "      \"\"\"Funcion auxiliar para actualizar el ranking\"\"\"\n",
        "      self.ranking[player1] += game_score[0]\n",
        "      self.ranking[player2] += game_score[1]\n",
        "\n",
        "    def plot_results(self):\n",
        "      \"\"\"\n",
        "      Plots a bar chart of the final ranking. On the x-axis should appear\n",
        "      the names of the sorted ranking of players participating in the\n",
        "      tournament. On the y-axis the points obtained.\n",
        "      \"\"\"\n",
        "      self.sort_ranking()\n",
        "\n",
        "      players = list(self.ranking.keys())  # Obtiene las claves (jugadores)\n",
        "      points = list(self.ranking.values())  # Obtiene los valores (puntos)\n",
        "\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.barh([player.name for player in players], [float(point) for point in points])  # Convertir los puntos a flotantes\n",
        "      plt.xlabel('Points')\n",
        "      plt.ylabel('Players')\n",
        "      plt.title('Final Ranking')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92h2VDkXDjzZ"
      },
      "source": [
        "Para testear la implementación, por ejemplo prueba a reproducir el campeonato de la sección \"3. One Tournament\" del juego de Nicky Case. En dicho campeonato se enfrentaban las 5 estrategias básicas (cooperador, desertor, tft, grudger y detective) en un torneo con 10 rondas por interacción y sin error. Los resultados que debes obtener son los siguientes:\n",
        " - Tit-For-Tat: ganador con 57 puntos\n",
        " - Grudger: 46 puntos\n",
        " - Detective: 45 puntos\n",
        " - Desertor: 45 puntos\n",
        " - Cooperador: 29 puntos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "Jl5Zb7KsDjzZ"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dilemma = Dilemma(2, -1, 3, 0)\n",
        "\n",
        "cooperator_player = Cooperator(dilemma, \"cooperator\")\n",
        "defector_player = Defector(dilemma, \"defector\")\n",
        "tft_player = Tft(dilemma, \"tft\")\n",
        "grudger_player = Grudger(dilemma, \"grudger\")\n",
        "detective_player = Detective4MovsTft(dilemma, \"detective\")\n",
        "el_t10 = T10(dilemma, \"T10\")\n",
        "\n",
        "all_players = (cooperator_player, defector_player, tft_player, grudger_player,\n",
        "               detective_player, el_t10)\n",
        "\n",
        "tournament = Tournament(all_players, n_rounds=10, error=0.0, repetitions=1)\n",
        "tournament.play()\n",
        "tournament.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CChTfIl9Djza"
      },
      "source": [
        "#### El módulo ```evolution```\n",
        "\n",
        "Implementa también la variante evolutiva del torneo, similar a la que se explica en la sección \"4. Repeated Tournament\" del juego de Nicky Case. Este módulo necesitará de inputs extra. Ten en cuenta lo siguiente:\n",
        " - La población inicial de jugadores no es directamente el input de jugadores que dé el usuario, sino que cada jugador tiene varios \"individuos\" o \"réplicas\" que jugarán su estrategia. Se propone dar al usuario dos opciones para definir esta población inicial:\n",
        "   - Si el usuario define el tamaño de la población total (```int```), se asume que cada jugador comienza con el mismo número de representantes. Divide ese número entre el número de jugadores (redondeado al entero más próximo) y así obtendrás el número de individuos inicial de cada estrategia.\n",
        "   - Si el usuario define una tupla de números de individuos (```list[int, ...]```), se asume que cada jugador tendrá el número de representantes que indique su índice dentro de dicha tupla.\n",
        " - Hay dos parámetros más que controlan el proceso evolutivo.\n",
        "   - En primer lugar, el porcentaje de individuos que se desea incluir en la selección natural tras cada ronda; esto es, el número de individuos *de la parte de abajo del ranking* que se van a eliminar y sustituir por los *individuos de la parte de arriba* (en caso de empate entre individuos, escoge al azar).\n",
        "   - En segundo lugar, el número de generaciones que se van a simular. Una generación es básicamente un \"Torneo de enfrentamiento directo\" + un \"Proceso de selección natural\".\n",
        "\n",
        "Deberás tomar una decisión sobre qué hacer en la selección natural cuando hay *empates*.\n",
        "\n",
        "A continuación se incluye la plantilla de desarrollo sugerida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "gj9E4kqJDjza"
      },
      "outputs": [],
      "source": [
        "class Evolution:\n",
        "\n",
        "    # Este método ya está implementado\n",
        "    def __init__(self, players: tuple[Player, ...],\n",
        "                       n_rounds: int = 100,\n",
        "                       error: float = 0.0,\n",
        "                       repetitions: int = 2,\n",
        "                       generations: int = 100,\n",
        "                       reproductivity: float = 0.05,\n",
        "                       initial_population: tuple[int, ...] | int = 100):\n",
        "        \"\"\"\n",
        "        Evolutionary tournament\n",
        "\n",
        "        Parameters:\n",
        "            - players (tuple[Player, ...]): tuple of players that will play the\n",
        "         tournament\n",
        "            - n_rounds (int = 100): number of rounds in each game\n",
        "            - error (float = 0.0): error probability (in base 1)\n",
        "            - repetitions (int = 2): number of games each player plays against\n",
        "         the rest\n",
        "            - generations (int = 100): number of generations to simulate\n",
        "            - reproductivity (float = 0.05): ratio (base 1) of worst players\n",
        "         that will be removed and substituted by the top ones in the natural\n",
        "         selection process carried out at the end of each generation\n",
        "            - initial_population (tuple[int, ...] | int = 100): list of\n",
        "         individuals representing each players (same index as 'players' tuple)\n",
        "         OR total population size (int).\n",
        "        \"\"\"\n",
        "\n",
        "        self.players = players\n",
        "        self.n_rounds = n_rounds\n",
        "        self.error = error\n",
        "        self.repetitions = repetitions\n",
        "        self.generations = generations\n",
        "        self.reproductivity = reproductivity\n",
        "\n",
        "        if isinstance(initial_population, int):\n",
        "            self.initial_population = [math.floor(initial_population\n",
        "                                       / len(self.players))\n",
        "                                       for _ in range(len(self.players))]\n",
        "        else:\n",
        "            self.initial_population = initial_population\n",
        "\n",
        "        self.total_population = sum(self.initial_population)\n",
        "        self.repr_int = int(self.total_population * self.reproductivity)\n",
        "\n",
        "        self.ranking = {copy.deepcopy(player): 0.0 for i, player in\n",
        "                        enumerate(self.players)\n",
        "                        for _ in range(self.initial_population[i])}\n",
        "\n",
        "\n",
        "    def natural_selection(self, result_tournament: dict[Player, float]) -> dict:\n",
        "      \"\"\"\n",
        "      Kill the worst guys, reproduce the top ones. Takes the ranking once a\n",
        "      face-to-face tournament has been played and returns another ranking,\n",
        "      with the evolutionary changes applied\n",
        "\n",
        "      Parameters:\n",
        "          - result_tournament: the 'tournament.ranking' kind of dict.\n",
        "\n",
        "      Results:\n",
        "          - Same kind of dict ranking as the input, but with the evolutionary\n",
        "          dynamics applied\n",
        "      \"\"\"\n",
        "\n",
        "      # no sé si asumir un diccionario ordenado pero por si acaso, luego se puede borrar\n",
        "      sorted_players = sorted(result_tournament.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      top_players = dict(sorted_players[:self.repr_int])\n",
        "\n",
        "      # reproduction\n",
        "      while len(top_players) < self.total_population:           # lógica de esto: priorizar que se mantenga el numero de jugadores\n",
        "        for player, score in list(top_players.items()):\n",
        "            if len(top_players) >= self.total_population:       # god forgive me for what i am about to code\n",
        "                break\n",
        "            cloned_player = copy.deepcopy(player)\n",
        "            top_players[cloned_player] = score        # creo que copiarlo con la misma score no es buena idea porque se sesga el ranking, pero con 0 da el mismo resultado, algo falla\n",
        "\n",
        "      return top_players\n",
        "\n",
        "\n",
        "\n",
        "    def count_strategies(self) -> dict[str, int]:\n",
        "        \"\"\"\n",
        "        Counts the number of played alive of each strategy, based on the\n",
        "        initial list of players. Should be computed analyzing the\n",
        "        'self.ranking' variable. Useful for the results plot/print (not needed\n",
        "        for the tournament itself)\n",
        "\n",
        "        Results:\n",
        "            - A dict, containing as values the name of the players and as\n",
        "         values the number of individuals they have now alive in the tournament\n",
        "        \"\"\"\n",
        "        strategy_count = {}\n",
        "\n",
        "        for player in self.ranking:\n",
        "            strategy_name = player.name\n",
        "            if strategy_name not in strategy_count:\n",
        "                strategy_count[strategy_name] = 1\n",
        "            else:\n",
        "                strategy_count[strategy_name] += 1\n",
        "\n",
        "        return strategy_count\n",
        "\n",
        "    def play(self, do_print: bool = False):\n",
        "        \"\"\"\n",
        "        Main call of the class. Performs the computations to simulate the\n",
        "        evolutionary tournament.\n",
        "\n",
        "        Parameters\n",
        "            - do_print (bool = False): if True, should print the ongoing\n",
        "            results at the end of each generation (i.e. print generation number,\n",
        "            and number of individuals playing each strategy).\n",
        "        \"\"\"\n",
        "\n",
        "        # initial population quantity for each player\n",
        "        count_evolution = {\n",
        "            player.name: [val] for player, val in zip(self.players, self.initial_population)\n",
        "        }\n",
        "\n",
        "        for generation in range(1, self.generations + 1):\n",
        "            # tournament for this generation\n",
        "            tournament = Tournament(self.players, n_rounds=self.n_rounds, error=self.error, repetitions=self.repetitions)\n",
        "            tournament.play()\n",
        "\n",
        "            # printing player count before evolving\n",
        "            current_count = self.count_strategies()\n",
        "            for player_name, count in current_count.items():\n",
        "                count_evolution[player_name].append(count)  # add count for current generation\n",
        "\n",
        "            # tournament result (accumulative between rounds)\n",
        "            result_tournament = {player: sum([score]) for player, score in tournament.ranking.items()}\n",
        "            self.ranking = result_tournament\n",
        "            self.ranking = self.natural_selection(result_tournament)\n",
        "\n",
        "            # print current results if do_print is True\n",
        "            if do_print:\n",
        "                print(f\"[🧬] Generation {generation}:\")\n",
        "                for player_name, count in current_count.items():\n",
        "                    print(f\"{player_name}: {count} individuals\")\n",
        "                print(\"\\n\")\n",
        "        return count_evolution\n",
        "\n",
        "\n",
        "    # Si quieres obtener un buen gráfico de la evolución, puedes usar este\n",
        "    # método si has seguido la pista indicada en la cabecera del método\n",
        "    # anterior. Ya está implementado, pero puede que necesites adaptarlo a tu\n",
        "    # código.\n",
        "    def stackplot(self, count_evolution: dict[str, list]) -> None:\n",
        "        \"\"\"\n",
        "        Plots a 'stackplot' of the evolution of the tournament\n",
        "\n",
        "        Parameters:\n",
        "            - count_evolution (dict[Player, list]): a dictionary containing as\n",
        "         keys the name of the strategies of the different players of the\n",
        "         tournament. Each value is a list, where the 'i'-th position of that\n",
        "         list indicates the number of individuals that player has at the end of\n",
        "         the 'i'-th generation\n",
        "         \"\"\"\n",
        "\n",
        "        COLORS = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n",
        "\n",
        "        for i, name in enumerate(count_evolution.keys()):\n",
        "            plt.plot([], [], label=name, color= COLORS[(i) % len(COLORS)])\n",
        "\n",
        "        plt.stackplot(list(range(self.generations + 1)),\n",
        "                      np.array(list(count_evolution.values())), colors=COLORS)\n",
        "\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKHxXZEJDjzb"
      },
      "source": [
        "Para testear el módulo anterior, puedes usar alguno de los experimentos mostrados en el juego de Nicky Case. Por ejemplo, un torneo con 5 TFT, 5 Desertores y 15 Cooperantes, sin error, con 10 rondas por interacción, con una reproductividad del 0.2, parece que en menos de 10 generaciones converge a una población dominada por TFT. Intenta replicar este resultado como se muestra a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWqS1N6SDjzb",
        "outputId": "0603e8e0-6e31-4a3c-974c-6757eb6291ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[🧬] Generation 1:\n",
            "cooperator: 15 individuals\n",
            "defector: 5 individuals\n",
            "tft: 5 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 2:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 3:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 4:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 5:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 6:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 7:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 8:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 9:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n",
            "[🧬] Generation 10:\n",
            "defector: 9 individuals\n",
            "tft: 8 individuals\n",
            "cooperator: 8 individuals\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'cooperator': [15, 15, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
              " 'defector': [5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
              " 'tft': [5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8]}"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dilemma = Dilemma(2, -1, 3, 0)\n",
        "\n",
        "cooperator_player = Cooperator(dilemma, \"cooperator\")\n",
        "defector_player = Defector(dilemma, \"defector\")\n",
        "tft_player = Tft(dilemma, \"tft\")\n",
        "\n",
        "all_players = (cooperator_player, defector_player, tft_player)\n",
        "\n",
        "evolution = Evolution(all_players, n_rounds=10, error=0.00, repetitions=1,\n",
        "                      generations=10, reproductivity=0.2,\n",
        "                      initial_population=(15, 5, 5))\n",
        "\n",
        "evolution.play(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G4esDTCDjzc"
      },
      "source": [
        "¡Enhorabuena! La primera parte de la práctica ya la has terminado. En la siguiente sección, tendrás que diseñar una estrategia para participar en un triple campeonato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U410RCwsDjzc"
      },
      "source": [
        "## Parte 2: diseño de una estrategia\n",
        "\n",
        "A continuación deberás implementar una estrategia que se enfrentará a las de tus compañeros un un torneo. Antes de describir las condiciones del campeonato, vamos a ver un concepto nuevo.\n",
        "\n",
        "Los juegos contra tus rivales tendrán un final **no determinista**. Esto quiere decir que el número de rondas que se van a jugar no se sabe con precisión. En lugar de eso, después de cada ronda, la partida tiene una cierta probabilidad de acabar (pequeña). ¿Por qué haremos esto? En verdad, el dilema del prisionero con una duración finita es un problema completamente distinto, ya que la proximidad del final de la interacción juega un papel crucial en las decisiones de las estrategias.\n",
        "\n",
        "Con el objetivo de dejar esta complejidad adicional fuera, jugaremos un número de rondas aleatorio: puedes pensar que tu estrategia va a jugar \"muchas veces\" contra cada rival. **En media, se jugarán 100 rondas** (puedes quedarte con ese número). Por supuesto, para que los resultados no estén determinados por el número de rondas jugadas, los puntos obtenidos en cada interacción se normalizarán por el número de rondas jugadas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMVTH7ZWDjzc"
      },
      "source": [
        "### Descripción del campeonato\n",
        "Ahora sí, vamos a ver las condiciones del torneo. Hay tres fases en esta competición:\n",
        " - Fase de enfrentamiento directo entre estrategias\n",
        " - Fase evolutiva\n",
        " - Fase evolutiva dentro del ecosistema completo del DPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwTEllmEDjzc"
      },
      "source": [
        "\n",
        "#### Fase de enfrentamiento directo entre estrategias\n",
        "\n",
        "Se jugará un torneo de *todos contra todos*: te enfrentarás a las estrategias de tus rivales dos veces. El resultado que obtengas en cada enfrentamiento se sumará a los ya obtenidos hasta el momento. Ganará la estrategia con más puntos al final de las interacciones. Las condiciones concretas son las siguientes:\n",
        " - El dilema del prisionero que se usará será el siguiente:\n",
        "\n",
        "<center>\n",
        "\n",
        "|      |  C    |  D    |\n",
        "|------|-------|-------|\n",
        "|   C  | 2, 2  | -1, 3  |\n",
        "|   D  | 3, -1  | 0, 0  |\n",
        "\n",
        "</center>\n",
        "\n",
        " - La probabilidad de acabar el enfrentamiento tras cada ronda $P_{end}$ se fija en 1%, con un máximo de rondas de 250.\n",
        " - La probabilidad de error $P_{error}$ se fija en 1%.\n",
        " - Se jugará 2 veces contra cada rival.\n",
        "\n",
        "Se asignarán los siguientes puntos según la posición final:\n",
        " - 5º y 6º clasificados: 4 puntos\n",
        " - 4º clasificado: 8 puntos\n",
        " - 3º clasificado: 12 puntos\n",
        " - 2º clasificado: 17 puntos\n",
        " - 1º clasificado: 24 puntos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0oAmY4vDjzd"
      },
      "source": [
        "\n",
        "#### Fase evolutiva\n",
        "\n",
        "Las estrategias se enfrentarán en un torneo evolutivo, en el que todos parten con el mismo número de representantes. Las condiciones para cada generación son las mismas que en la fase anterior.\n",
        "\n",
        "La reproductividad se manejará de forma ligeramente distinta a lo visto en secciones anteriores. Ahora, en sucesivas generaciones, la proporción de individuos de un jugador frente al total será la misma que la proporción de puntos obtenidos por todos esos individuos frente al total de puntos obtenidos por todas las estrategias. Por ejemplo, imagina que una determinada generación tienes 17 individuos de un total de 100. Estos individuos, en suma han obtenido un total de 210 puntos. Además, la suma de puntos obtenidos por todas las individuos de la población es 1000 puntos. Por tanto, en la siguiente generación dispondrás de 21 individuos.\n",
        "\n",
        "Aquí la evaluación de los ganadores puede ser complicada. En principio, el orden en el que se extingan las estrategias indicará el ranking de esta fase. No obstante, en ocasiones surgen situaciones difíciles de evaluar (por ejemplo, comportamientos cíclicos de dominancia de varias estrategias). En este tipo de escenarios, serán los profesores quienes decidan qué estrategias han sido las más exitosas y su orden. Los puntos que pueden obtenerse en esta fase son los siguientes:\n",
        " - 3º clasificado: 12 puntos\n",
        " - 2º clasificado: 17 puntos\n",
        " - 1º clasificado: 24 puntos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw1W28JNDjzd"
      },
      "source": [
        "#### Fase evolutiva dentro del ecosistema completo del DPI\n",
        "\n",
        "En esta fase, tu estrategia se enfrentará en un gran torneo evolutivo, donde estarán incluidas las estrategias de tus compañeros, pero también hasta 50 estrategias adicionales del ecosistema del DPI. Estarán las que ya conoces (TFT, Grudger, etc.), y también las estrategias que se han mostrado más exitosas en las últimas publicaciones científicas. De nuevo, el criterio general para establecer el ranking entre nuestras estrategias será: \"cuanto más tarde se extingan tus individuos, más alto estarás en el ranking\". Al haber tantas estrategias involucradas, esta parte del campeonato es la más susceptible a presentar situaciones difíciles de evaluar, por lo que en última instancia serán los profesores quienes, partiendo de criterios objetivos, seleccionen las tres estrategias más exitosas, que se llevarán:\n",
        " - 3º clasificado: 4 puntos\n",
        " - 2º clasificado: 8 puntos\n",
        " - 1º clasificado: 12 puntos\n",
        "\n",
        "Los puntos de las tres fases se sumarán y **el grupo ganador obtendrá 0.4 puntos extra en la evaluación de la asignatura**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKDATcc9Djzd"
      },
      "source": [
        "### Implementación de tu estrategia\n",
        "\n",
        "A la hora de implementar tu estrategia ten en cuenta lo siguiente:\n",
        " - Si has seguido la plantilla de desarrollo de la primera parte de la práctica, tu estrategia deberá ser implementada como un subclase de la clase abstracta ```Player()```. En particular, debes implementar cuidadosamente su método ```strategy()```.\n",
        " - No olvides darle un buen nombre a tu estrategia.\n",
        " - Igualmente, mientras implementas, incluye todo los comentarios que puedan ayudar a los profesores a evaluar positivamente tu estrategia. Por ejemplo: \"*he visto que mi estrategia funcionaba mal contra Desertores, por eso incluyo el siguiente bloque de código que pretende...*\".\n",
        " - **¡IMPORTANTE!** Incluye información suficientemente extensa para explicar tus decisiones de diseño, por qué presentas esa estrategia y no otra, cuáles son los resultados que has observado en las pruebas que has hecho, qué problemas te has encontrado y cómo los has solucionado, etc.\n",
        "\n",
        "A continuación se incluye un ejemplo de implementación de una estrategia. De hecho, este ejemplo será la estrategia de los profesores, ¡y participará en el campeonato! Así que ya sabes una de las estrategias que va a estar presente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "R5XM5SjTDjze"
      },
      "outputs": [],
      "source": [
        "class Destructomatic(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"\"):\n",
        "        \"\"\"...\"\"\"\n",
        "        super().__init__(dilemma, name)\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"\n",
        "        A \"random\" friend of a tf2t\n",
        "        Starts cooperating, very forgiving\n",
        "        It defects randomly ¿? =D\n",
        "        \"\"\"\n",
        "\n",
        "        turns = len(self.history)\n",
        "\n",
        "        if turns < 2:  # I don't want any trouble, my friend (at least now)\n",
        "            return C\n",
        "\n",
        "        # [I need some data to draw up my plan...] \"I'm very forgiving :)\"\n",
        "        if turns < 10:\n",
        "            if opponent.history[-2:] != [D, D]:\n",
        "                return C\n",
        "            else:\n",
        "                return D\n",
        "\n",
        "        count_D_opponent = opponent.history.count(D)  # Num. of defections of the opponent\n",
        "\n",
        "        # This is my rule, dude\n",
        "        if count_D_opponent / turns > random.random():\n",
        "            return D\n",
        "\n",
        "        # Ok, but in general I'm very forgiving, as you see\n",
        "        if opponent.history[-2:] != [D, D]:  # tf2t condition\n",
        "            return C\n",
        "\n",
        "        # Otherwise -> opponent.history[-2:] == [D, D]\n",
        "        return D\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YrHbGtdDjzf"
      },
      "source": [
        "#### Espacio para el desarrollo de tu estrategia\n",
        "\n",
        "Implementa tu estrategia en la plantilla de la siguiente celda, modificándola a conveniencia. Puedes añadir más métodos si lo necesitas. Incluye una descripción explicando el proceso que has seguido para diseñarla: usa el docstring de tu clase o una celda de texto adicional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "U_uGTFc6Djzf"
      },
      "outputs": [],
      "source": [
        "class T10(Player):\n",
        "\n",
        "    def __init__(self, dilemma: Dilemma, name: str = \"T10\"):\n",
        "        \"\"\"T10\"\"\"\n",
        "        self.name = name\n",
        "        self.dilemma = dilemma\n",
        "        self.history  = []\n",
        "        self.point = 0\n",
        "\n",
        "\n",
        "    def strategy(self, opponent: Player) -> int:\n",
        "        \"\"\"Cooperates first, then use TfT if we are wining, else we change to Grudger or Win–stay, lose–switch if we are losing\"\"\"\n",
        "\n",
        "        turn = len(self.history)             # Current Turn\n",
        "        count_D = opponent.history.count(D)  # Num. of defections of the oponent\n",
        "        count_C = opponent.history.count(C)  # Num. of cooperations of the oponent\n",
        "        #Point calculator\n",
        "        if len(self.history) > 0 and len(opponent.history) > 0:\n",
        "            self.point += pd.evaluate_result(self.history[-1], opponent.history[-1])[0]\n",
        "\n",
        "        #We start at turn 1 cooperating\n",
        "        if turn == 0 or turn == 1:\n",
        "            self.history.append(C)\n",
        "            return C  \n",
        "        \n",
        "        if turn == 9 and (opponent.history[-1] == 0 and opponent.history[-2] == 0 and opponent.history[-3] == 0):\n",
        "            self.history.append(D)\n",
        "            return D \n",
        "        \n",
        "        #If at mid round we are losing we use Grudger or Win–stay until end match\n",
        "        if 10 >= turn >= 5 and self.point <= 10:\n",
        "            self.point =0\n",
        "            if count_D > count_C:\n",
        "            #Grudger\n",
        "                action = C\n",
        "                if len(opponent.history) > 0 and len(self.history) > 0:\n",
        "                    if opponent.history[-1] == D or self.history[-1] == D:\n",
        "                        action = D\n",
        "                return action\n",
        "            \n",
        "            else:\n",
        "                # Win–stay, lose–switch\n",
        "                last_round = self.history[-1], opponent.history[-1]\n",
        "                if last_round[0] == C and last_round[1] == D:\n",
        "                    self.history.append(D)\n",
        "                    return D # Switch before lose\n",
        "                else:\n",
        "                    self.history.append(C)\n",
        "                    return C # Stil cooperates before winning\n",
        "        \n",
        "        #Until that (if we are winning), we follow TfT\n",
        "        if 5 >= turn > 1 or (10 >= turn > 5 and self.point > 10):\n",
        "            opponents_last_action = opponent.history[-1]\n",
        "            self.history.append(opponents_last_action)\n",
        "            return opponents_last_action\n",
        "\n",
        "        else:\n",
        "            opponents_last_action = opponent.history[-1]\n",
        "            self.history.append(opponents_last_action)\n",
        "            return opponents_last_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku6fakGHDjzf"
      },
      "source": [
        "Para la entrega, explica en detalle el proceso de diseño de tu estrategía, por qué has decidido presentar esa estrategia y no otras con las que hayas probado, qué consideraciones has tenido en cuenta al diseñarla, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selected strategy and proposed considerations ###\n",
        "\n",
        "##### Study of previous strategies #####\n",
        "\n",
        "Since the inception of the prisoner's dilemma many strategies have gained importance over the years along with a unique approach to each of them. According to Robert Axelrod in several of his investigations carried out from the study of many game strategies, the best ones shared a series of characteristics that made them have a higher final score, which we have taken into consideration for the elaboration of our new strategy. These features are the following ones\n",
        "\n",
        "- Promotion of Cooperation: Numerous contemporary strategies emphasize the optimization of cooperative interactions over defection, with the primary goal of avoiding initiation of defection by these strategies.\n",
        "\n",
        "- Retaliatory Mechanisms: Axelrod argued that an effective strategy should not exhibit unwavering optimism. It should consider the possibility that a defection could exploit strategies prioritizing collaboration over defection.\n",
        "\n",
        "- Forgiveness Dynamics: The incorporation of forgiveness into algorithms grants them the capacity to pardon an opponent for ceasing to defect. This dynamic fosters an environment conducive to mutual collaboration and maximizes overall payoffs.\n",
        "\n",
        "- Clarity Requirement: This attribute advises strategies to be easily predictable, promoting enhanced mutual cooperation even when uncertainties exist regarding the intentions of the opponent.\n",
        "\n",
        "As an example of strategies that employ some of these four conditions, the TFT strategy has been considered to be the most effective one using the \"Nice\", \"Retaliate\" and \"Forgive\" features.\n",
        "\n",
        "\n",
        "Numerous strategies proposed throughout the prisoner's dilemma are based on variations or combinations of other strategies, following a number of specific criteria. A notable example of this is the Grudger strategy, which transitions from a cooperative stance to a fully deserting one in the event that the opponent decides to defect on a single occasion.\n",
        "\n",
        "\n",
        "With this in mind we will aim to make our own strategy follow at least three of the four conditions for best performance.\n",
        "\n",
        "##### Human experimentation #####\n",
        "\n",
        "In order to draw conclusions and explore more effective approaches to determine an optimal strategy, we decided to recruit volunteers to repeat the prisoner's dilemma under the same rules proposed in practice. During this testing phase, we conducted ten rounds using the same point system, which led to a number of results:\n",
        "\n",
        "- Throughout the experiment, several participants opted for a defection strategy. Even despite the persistence of this strategy, many showed a willingness to \"forgive\" spontaneously, despite repeatedly disloyal behavior on the part of the opponent.\n",
        "\n",
        "- From the middle of the experiment (either turn 5 or 6), participants decided to change their strategies based on their previous learning, opting for completely different strategies.\n",
        "\n",
        "- In situations where both players cooperated in one turn, many of them chose to defect in the next two turns in order to score more points, resulting in several rounds of defection on both sides.\n",
        "\n",
        "With these observations in mind, we have decided to incorporate several of the behaviors identified during the test and fuse them into a strategy that integrates some of these principles. The goal is to develop our own unique strategy capable of rivaling some of the most commonly used strategies in the dilemma.\n",
        "\n",
        "### Description of the strategy ###\n",
        "\n",
        "The T1O strategy in the Prisoner's Dilemma is a dynamic approach that evolves throughout the game. Initially cooperative, the player employs a Tit for Tat strategy for several turns, reflecting the last action of the opponent. As the game evolves, the strategy adapts according to the accumulated points and historical actions. If the points exceed a threshold or the game reaches a certain turn, the player continues with TFT. However, if the points are below a specified level, the strategy can change to continued defection or employ a \"Win–stay, lose–switch\" tactic, defecting only if the opponent defected in the previous round. This blended strategy combines cooperative and retaliatory elements, showing a strategic response to diverse game conditions.\n",
        "\n",
        "For last, we have a secret condition due to the experienced acquired in experimentation, in the final round if we have cooperated three times in the past turns, we will defect no matter what, demonstrating efficiency against \"nice\" strategies.\n",
        "\n",
        "-----\n",
        "\n",
        "And why we change to WSLS and ADFL?\n",
        "\n",
        "According to Robert Axelrod in Evolution of Cooperation, \"If the game is played a known finite number of times, the players still have no incentive to cooperate.\" meaning that however the strategy we have, it is impossible to determine one hundred percent that the opponent will cooperate, either because of the existence of error, past patterns or predictable behaviors. Considering the consistent pattern of defections observed early in the rounds, we believe the most viable strategy is to defect in all subsequent rounds.\n",
        "\n",
        "### References ###\n",
        "\n",
        "Nowak M, Sigmund K. A strategy of win-stay, lose-shift that outperforms tit-for-tat in the Prisoner's Dilemma game. Nature. 1993 Jul 1;364(6432):56-8.\n",
        "\n",
        "Milinski M. Evolutionary biology. Cooperation wins and stays. Nature. 1993 Jul 1;364(6432):12-3.\n",
        "\n",
        "82 S. Cal. L. Rev. 209 (2008-2009) Beyond the Prisoners' Dilemma: Coordination, Game Theory, and Law \n",
        "\n",
        "Axelrod, R. 1984. The Evolution of Cooperation. New York: Basic Books.\n",
        "\n",
        "Rapoport, A. and Chammah, A.M. 1965. Prisoner’s Dilemma. Ann Arbor: University of Michigan Press.\n",
        "\n",
        "Nowak, M.; Sigmund, K. (July 1, 1993). \"A strategy of win-stay, lose-shift that outperforms tit-for-tat in the Prisoner's Dilemma game\". Nature. 364 (6432): 56–58\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "cgBX2mKqDjzg"
      },
      "outputs": [],
      "source": [
        "# Haz todas las pruebas que necesites aquí.\n",
        "# Por ejemplo:\n",
        "#\n",
        "# game = Game(Destructomatic(dilemma, \"destr\"), Tft(dilemma, \"tft\"),\n",
        "#             dilemma, n_rounds=10, error=0.1)\n",
        "# game.play(do_print=True)\n",
        "#\n",
        "# O también:\n",
        "#\n",
        "# dilemma = Dilemma(13, 0, 20, 4)\n",
        "# participants = (Destructomatic(dilemma, \"destr\"),\n",
        "#                 Cooperator(dilemma, \"coop1\"),\n",
        "#                 Defector(dilemma, \"defect\"),\n",
        "#                 Cooperator(dilemma, \"coop2\"),\n",
        "#                 Tft(dilemma, \"tft\"),\n",
        "#                 Detective4MovsTFT(dilemma, \"detect\"))\n",
        "#\n",
        "# tournament = Tournament(participants, dilemma, n_rounds=100, error=0.01,\n",
        "#                         repetitions=2)\n",
        "# tournament.play()\n",
        "# tournament.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjN-2JJ8Djzg"
      },
      "source": [
        "### Rúbrica de calificación de la práctica\n",
        "\n",
        "PARTE 1:\n",
        " - **[4/10] Funcionamiento del código**: el código descrito en la *Parte 1* de la práctica debe funcionar correctamente, sin *bugs*, permitiendo reproducir todas las funcionalidades descritas.\n",
        " - **[2/10] Limpieza y claridad de estilo**.\n",
        "  \n",
        "PARTE 2:\n",
        " - **[1/10] Implementación de una estrategia correcta**: que funcione sin errores y que esté programada con claridad.\n",
        " - **[3/10] Originalidad de la estrategia y trabajo de exploración llevado a cabo**: el tipo de estrategia presentada, junto con las explicaciones que hayas aportado en comentarios y demás, pondrá de manifiesto el trabajo de experimentación que has llevado a cabo con tu estrategia. Este item pretende evaluar este aspecto.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LEXbkD7Djzg"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1beef6bdf7cb3edc825034b485381c9781f3bd7bbda8256bc912318d884443a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
